[{"authors":["admin"],"categories":null,"content":"I am a Postdoctoral Research Fellow at the Athinoula A. Martinos Center for Biomedical Imaging at Massachusetts General Hospital and Harvard Medical School, working in Ciprian Catana\u0026lsquo;s PET-MR lab.\nMy main research interest is on tomographic image reconstruction and parametric maps estimation from PET data. I am an author of several peer-reviewed publications and proceedings, mainly on: models for 4D PET image reconstruction incorporating kinetic modeling and clustering; strategies for improving maps‚Äô SNR and accelerating voxelwise fitting via GPU parallelization; PET reconstruction algorithms for non-Poisson data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Postdoctoral Research Fellow at the Athinoula A. Martinos Center for Biomedical Imaging at Massachusetts General Hospital and Harvard Medical School, working in Ciprian Catana\u0026lsquo;s PET-MR lab.\nMy main research interest is on tomographic image reconstruction and parametric maps estimation from PET data.","tags":null,"title":"Michele Scipioni","type":"authors"},{"authors":[],"categories":["teaching"],"content":"","date":1584151470,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584151470,"objectID":"440172360d56dd4b2f6b406264de3c94","permalink":"/talk/intro-to-matlab-whyehow/","publishdate":"2020-03-13T22:04:30-04:00","relpermalink":"/talk/intro-to-matlab-whyehow/","section":"talk","summary":"","tags":["teaching","talks","invited","Matlab","coding"],"title":"Introduction to Matlab ","type":"talk"},{"authors":["Michele Scipioni","Maria Filomena Santarelli","Assuero Giorgetti","Vincenzo Positano","Luigi Landini"],"categories":null,"content":"","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"72eb80efc4344f9a19b7b116ea32e837","permalink":"/publication/nb-mlem/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/nb-mlem/","section":"publication","summary":"*Purpose* Positron emission tomography (PET) image reconstruction is usually performed using maximum likelihood (ML) iterative reconstruction methods, under the assumption of Poisson distributed data. Pre-correcting raw measured counts, this assumption is no longer realistic. The goal of this work is to develop a reconstruction algorithm based on the Negative Binomial (NB) distribution, which can generalize over the Poisson distribution in case of over-dispersion of raw data, that may occur if sinogram pre-correction is used. *Methods* The mathematical derivation of a Negative Binomial Maximum Likelihood Expectation-Maximization (NB-MLEM) algorithm is presented. A simulation study to compare the performance of the proposed NB-MLEM algorithm with respect to a Poisson-based MLEM (P-MLEM) method was performed, in reconstructing PET data. The proposed NB-MLEM reconstruction was tested on a real phantom and human brain data. *Results* For the property of NB distribution, it is a generalization of the conventional P-MLEM: for not over dispersed data, the proposed NB-MLEM algorithm behaves like the conventional P-MLEM; for over-dispersed PET data, the additional evaluation of the dispersion parameter after each reconstruction iteration leads to a more accurate final image with respect to P-MLEM. *Conclusions* A novel approach for PET image reconstruction from pre-corrected data has been developed, which exhibits a statistical behavior that deviates from the Poisson distribution. Simulation study and preliminary tests on real data showed how the NB-MLEM algorithm, being able to explain the over-dispersion of pre-corrected data, can outperform other algorithms that assume no over-dispersion of pre-corrected data, while still not accounting for the presence of negative data, such as P-MLEM.","tags":null,"title":"Negative binomial maximum likelihood expectation maximization (NB-MLEM) algorithm for reconstruction of pre-corrected PET data","type":"publication"},{"authors":["Michele Scipioni"],"categories":null,"content":"","date":1564099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564099200,"objectID":"dcbcd5b155f12803b101b51fd0f3ebbe","permalink":"/publication/embc19_directclustering/","publishdate":"2019-07-26T00:00:00Z","relpermalink":"/publication/embc19_directclustering/","section":"publication","summary":"Dynamic positron emission tomography (dPET) is known for its ability to extract spatiotemporal information of a radio tracer in living tissue. In this paper, a novel direct reconstruction framework is presented, which include concurrent clustering as a potential aid in addressing high levels of noise typical of voxel-wise kinetic modeling. Core assumption is that the imaged volume is formed by a finite number of different functional regions, and that voxel-wise time courses are determined by the functional cluster they belong to. Probabilistic Graphical Modeling (PGM) theory is used to describe the problem, and to derive the inference strategy. The proposed iterative estimation scheme provides concurrent estimate of kinetic parameter maps, activity images, and segmented clusters. Simulation studies and exploratory application to real data are performed to validate the proposal.","tags":null,"title":"Direct 4D PET reconstruction with discrete tissue types","type":"publication"},{"authors":null,"categories":null,"content":"Abstract\nDynamic positron emission tomography (dPET) is known for its ability to extract spatiotemporal information of a radio tracer in living tissue. In this paper, a novel direct reconstruction framework is presented, which include concurrent clustering as a potential aid in addressing high levels of noise typical of voxel-wise kinetic modeling. Core assumption is that the imaged volume is formed by a finite number of different functional regions, and that voxel-wise time courses are determined by the functional cluster they belong to. Probabilistic Graphical Modeling (PGM) theory is used to describe the problem, and to derive the inference strategy. The proposed iterative estimation scheme provides concurrent estimate of kinetic parameter maps, activity images, and segmented clusters. Simulation studies and exploratory application to real data are performed to validate the proposal.\n","date":1564099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564099200,"objectID":"3a5111e3437e9b67d962f76193e8f106","permalink":"/talk/embc2019/","publishdate":"2019-07-26T00:00:00Z","relpermalink":"/talk/embc2019/","section":"talk","summary":"Abstract\nDynamic positron emission tomography (dPET) is known for its ability to extract spatiotemporal information of a radio tracer in living tissue. In this paper, a novel direct reconstruction framework is presented, which include concurrent clustering as a potential aid in addressing high levels of noise typical of voxel-wise kinetic modeling.","tags":null,"title":"Direct 4D PET reconstruction with discrete tissue types","type":"talk"},{"authors":["Michele Scipioni","Stefano Pedemonte","Maria Filomena Santarelli","Luigi Landini"],"categories":null,"content":"","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561852800,"objectID":"ed84021e84a28ea25b2d169a27ba1f78","permalink":"/publication/pgm-pet/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/publication/pgm-pet/","section":"publication","summary":"In the context of dynamic emission tomography, the conventional processing pipeline consists of independent image reconstruction of single time frames, followed by the application of a suitable kinetic model to time-activity curves (TACs) at the voxel or region-of-interest level. Direct 4D PET reconstruction, by contrast, seeks to move beyond this scheme and incorporate information from multiple time frames within the reconstruction task. Established direct methods are based on a deterministic description of voxelwise TACs, captured by the chosen kinetic model, considering the photon counting process the only source of uncertainty. In this work, we introduce a new probabilistic modeling strategy based on the key assumption that activity time course would be subject to uncertainty even if the parameters of the underlying dynamic process are known. This leads to a hierarchical model, which we formulate using the formalism of Probabilistic Graphical Modeling. The inference is addressed using a new iterative algorithm, in which kinetic modeling results are treated as prior expectation of activity time course, rather than as a deterministic match, making it possible to control the trade-off between a data-driven and a model-driven reconstruction. The proposed method is flexible to an arbitrary choice of (linear and nonlinear) kinetic models, it enables the inclusion of arbitrary (sub)differentiable priors for parametric maps, and it is simple to implement. Computer simulations and an application to a real patient scan show how the proposed method is able to generalize over conventional indirect and direct approaches, providing a bridge between them by properly tuning the impact of the kinetic modeling step on image reconstruction. ","tags":null,"title":"Probabilistic Graphical Models for dynamic PET: a novel approach to direct parametric map estimation and image reconstruction","type":"publication"},{"authors":null,"categories":null,"content":" Utteranc.es is a lightweigth comments widget, which allows you to use Github Issues for blog comments. It\u0026rsquo;s open source, looks clean, comments are stored on Github, and even comes with a dark theme. Sure, you need to sign with Github, but that\u0026rsquo;s fine since most coders already have an account.\nInstallation steps  You will, obviously, need to have your website hosted on GitHub, in a public repository, in order to utterances to work as intended. Install utterances app on that repo. You have a choice to install the app on every (current and future) repo, but I don\u0026rsquo;t think you will have any need for this. Usually you will have a (public) repo with a name like \u0026lt;username\u0026gt;.github.io: select this from the drop-down menu that will appear:   Go to utterances web-app and fill the form as requested. It will generate (at the bottom of the page) a custom html that you could copy\u0026amp;paste in your blog template. you will require just a couple of information:\n name of the repository : usually, it will be something like \u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io (or more generally owner/repo) label: as the comments will be managed via GitHub Issue system, you will need to set-up a proper label to indentify those issues created by utterances (in case you have other normal issues as well in you repo) theme: your choice of a light or dark theme, according to the overall style of your current blog template    Copy to your clipboard.\n  [The following will apply only to Hugo Academic template, but it you are a little bit tech-savy you will find a way to make it work with whatever template you are using, even a custom one] Go to the folder in which the Hugo surce of your blog is hosted, and navigate to themes/academic/layouts/partials\n  Open the source file comments.html and replace everything in it with utterances script code:\n  \u0026lt;script src=\u0026quot;https://utteranc.es/client.js\u0026quot; repo=\u0026quot;mscipio/mscipio.github.io\u0026quot; issue-term=\u0026quot;pathname\u0026quot; label=\u0026quot;Comment\u0026quot; theme=\u0026quot;github-light\u0026quot; crossorigin=\u0026quot;anonymous\u0026quot; async\u0026gt; \u0026lt;/script\u0026gt;  That\u0026rsquo;s it! Now, check that everything is set-up correctly in the post template file, which can be found at themes/academic/layouts/_defaults/single.html. Towards the end you should find something similar to this:  \u0026lt;div class=\u0026quot;article-container\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; {{ partial \u0026quot;comments.html\u0026quot; . }} \u0026lt;/div\u0026gt;   Just in case you wanted to activate comments also for the Publications section provided by the Academic template, just copy the code snippet above and past it in themes/academic/layouts/publications/single.html, towards the end, just before the \u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; tag.\n  Save and deploy, as you normally would.\n  Voila! Check it out below! üëá\n ","date":1549897200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549897200,"objectID":"78933bac4c61b784ac10a72060ba3f40","permalink":"/post/utterances-comment-engine/","publishdate":"2019-02-11T16:00:00+01:00","relpermalink":"/post/utterances-comment-engine/","section":"post","summary":"A lightweight comments widget built on GitHub issues. Use GitHub issues for blog comments!\n","tags":["github","utteranc.es","blog"],"title":"Add comments to your Hugo-Academic blog in 10 minutes, using Utteranc.es","type":"post"},{"authors":null,"categories":null,"content":"Nowadays the ability to write codes has become an essential skill in technical and scientific disciplines. Either you like it or not, during your studies you will find yourself doing assignments, solving equations or bigger \u0026lsquo;problems\u0026rsquo; of your projects with some sort of coding. And, if you think of going for higher studies and doing some extensive research, then writing codes is a must know skill for you.\nQuite often, students will become familiar with scientific programming (note that I am not specifically refferring to CS students, focused on general purpose coding and programming) through MATLAB. The simple reason for that is that MATLAB has been there for scientific computing for a long while, and it has become a legacy language or tool for the scientific community. Engineers and Scientists always needed a programming language that expresses matrix and array mathematics directly, and then MATLAB (matrix laboratory) came into existence. MATLAB is a math and matrix oriented language comes with different types of specialized toolboxes (you have to pay for toolbox) for several purposes e.g. modelling economic data, image analysis or driving a robot. These toolboxes are professionally developed, rigorously tested and well documented for scientific and engineering applications. And that‚Äôs why you pay the price for it.\nMATLAB has a solid amount of functions amd an extraordinarily good documentation to start learning, and a large scientific community who have either answered the questions that are going to be asked or will be answered by someone as you post them in the MATLAB Central. There are 365,000 contributors, 120 questions are answered and 25,000 sample scripts or codes are downloaded per day. It has toolboxes for computational biology, computational finances, control systems, data science, image processing and computer vision, machine learning, physical modelling and simulation, robotics, signal processing and communications and IOT.\nOn the other side, we have Python, whcih is a much younger programming language, whose history of scientific computing packages, e.g. SciPy, NumPy, have not been antiquated. Moreover, in Python you often have to rely on community-authored packages for scientific and engineering usages. Calling Python as an alternative to MATLAB is technically incorrect. It is a general purpose programming language, which you to develop fully fledged apps and software tools, and to create applications using any of the major GUI libraries (e.g. Qt), use OpenGL, drive your USB port, etc.\nBeing a free, cross-platform, general-purpose and high-level programming language, lots of people are now adopting Python. IDES like pycharm, ipython notebook, jupyter notebook an distributions like anaconda has made python far more usable for researchers. As a result of this popularity, plenty of Python scientific packages have become available with extensive documentation for data visualization, machine learning, natural language processing, complex data analysis and more. For example, scikit-learn includes start-of-the-art ‚ÄòMachine Learning‚Äô approaches with very good documentation and tutorials.\nSometimes, choosing between MATLAB and Python is a personal matter, or it could be task-specific. Other times, you may be forced to opt for Python. Personally, there are some fundamental issues that made me search for an alternative to MATLAB. I think the most fundamental problem with Matlab is its commercial nature, and this is the basis for several issues:\n The algorithms are proprietary, which means you (most of the times) can not see the code of the algorithms you are using and have to trust that Matlab implemented it right. Obviously, Matlab is expensive. It makes portability more difficult. The portability solution (the Matlab Component Runtime (MCR)) works fine, but Matlab had to take great care that one cannot use it to do generic Matlabing with it. Maybe this is the reason that the application must be exactly the same version as the installed MCR, which can be a nuisance considering that Matlab releases a new version every 6 months. The proprietary nature also makes it hard, if not impossible, for 3th parties to extend or create tools for Matlab.  Of course, Matlab has its advantages too:\n It has a solid amount of functions. It mights also be easier to use for beginners, because the package includes all, while in Python you need to install extra packages and an IDE. It has a large scientific community. It is used on many universities (but few companies have the money to buy a license).  Last point is even more important if you consider the possibility of you working in academic research. It is not so unlikely that your colleagues are more familiar using MATLAB than Python, or that code examples ot functions released alongside published research articles will be written in MATLAB. Moreover, MATLAB supports writing complex (and computationally expensive) function in C/C++ source files, which are later compiled in a proprietary binary format called MEX.\nLong story short:\n you are a hardcore Python user and supported but find yourself dealing with MATLAB-friendly colleagues; you need to use a function which is shipped as a compiled binary MEX file (meaning that even if you wanted, you cannot read and translate the source to Python, or recompile the C/C++ source in such a way it is possible to call it from Python); or simply you like really much how a tool has been implemented in MATLAB (e.g. functions of the Statistical Toolbox, or the Optimization Toolbox, which are really well developed and documented) and you want to directly use them, instead of looking for native Python alternative.  If you recognize yourself into one of the previous categories, in the remainder of this post we are going to see a couple of strategies you can use to call MATLAB functions from you Python code, in such a way that they will behave like native Python code, accepting inputs and providing outputs directly into Python current workspace.\nMATLAB API for Python To the MATLAB¬Æ Engine API for Python¬Æ you will need to have a copy of MATLAB installed in you system. There is no workaround for this, as far as I know, and this is a consequence of MATLAB being a proprietary software. This API supports almost every version of Python, and requires CPython to be installed on your system, in order to use the referencing of inputs and outputs required to exchange arguments between the two worlds.\nIf you satisfies this requirements, the installation of the API is very simple, and it is done as you would do for every Python source code library. On Linux it sounds like this:\ncd \u0026quot;matlabroot/extern/engines/python\u0026quot; python setup.py install  where matlabroot is the path where you installed MATLAB on your system.\nThat\u0026rsquo;s it!\nThe API provides a Python package named matlab that enables you to call MATLAB functions from Python. You install the package once, and then you can call the engine in your current or future Python sessions. You can import this newly installed package by importing it into your current Python session:\nimport matlab.engine eng = matlab.engine.start_matlab()  IF you want to keep things separated, and you need to have different sessions/workspaces for MATLAB, within you workflor, you can simply start multiple engines, which won\u0026rsquo;t communicate with each other:\neng1 = matlab.engine.start_matlab() eng2 = matlab.engine.start_matlab()  To stop a matlab engine you can either quit your current Python session, or explicitly arrest the engine itself:\neng1.exit eng2.quit()  Call MATLAB (built-in) functions from Python You can call any MATLAB function directly and return the results to Python. This holds as long as the function can be found in MATLAB\u0026rsquo;s path (we will come beck to this shortly).\nFor example, to determine if a number is prime, use the engine to call the isprime function.\ntf = eng.isprime(37) print(tf) print(type(tf))  True \u0026lt;type 'bool'\u0026gt;  This was a simple one: the MATALB function we call produced only one output, and it was a \u0026lsquo;scalar\u0026rsquo; (actually boolean) output, not an array of some type.\nWhen you call a function with the engine, by default the engine returns a single output argument. If you know that the function can return multiple arguments, you will need to use the nargout argument to specify the number of output arguments.\nAs an example, to determine the greatest common denominator of two numbers, use the gcd function, by setting nargout to return the three output arguments from gcd:\nt = eng.gcd(100.0,80.0,nargout=3) print(t) print(type(t))  (20.0, 1.0, -1.0) \u0026lt;type 'tuple'\u0026gt;  Transfering variables from Python to MATLAB workspace When you start the engine, it provides an interface to a collection of all MATLAB variables. This collection, named workspace, is implemented as a Python dictionary that is attached to the engine:\n The name of each MATLAB variable becomes a key in the workspace dictionary. The keys in workspace must be valid MATLAB identifiers (e.g., you cannot use numbers as keys).  You can add variables to the engine workspace in Python, and then you can use the variables in MATLAB functions:\n# variable x in Python workspace x = 4.0 # a new variable called y is added to MATLAB workspace, and is value is set to be equal to Python's x eng.workspace['y'] = x # we can use variable y while calling MATLAB functions, ad MATLAB is aware of all the variable availabe in its workspace a = eng.eval('sqrt(y)') print(a)  2.0  In this example, x exists only as a Python variable. Its value is assigned to a new entry in the engine workspace, called y, creating a MATLAB variable. You can then call the MATLAB eval function to execute the sqrt(y) statement in MATLAB and return the output value, 2.0, to Python.\nUse MATLAB Arrays in Python Usually, while working with MATLAB, we are interested in performing complex operations on arrays. The matlab package provides constructors to create MATLAB arrays in Python. The MATLAB Engine API for Python can pass such arrays as input arguments to MATLAB functions, and can return such arrays as output arguments to Python.\nYou can create arrays of any MATLAB numeric or logical type from Python sequence types, as follows:\na = matlab.double([1,4,9,16,25]) b = eng.sqrt(a) print(b) print(type(b))  [[1.0,2.0,3.0,4.0,5.0]] \u0026lt;class 'matlab.mlarray.double'\u0026gt;  The engine returns b, which is a 1-by-5 matlab.double array.\nThe same applies if we want to create a multidimensional array. The magic function returns a 2-D matlab.double array to Python.\na = eng.magic(6) for x in a: print(x) print(type(a))  [35.0,1.0,6.0,26.0,19.0,24.0] [3.0,32.0,7.0,21.0,23.0,25.0] [31.0,9.0,2.0,22.0,27.0,20.0] [8.0,28.0,33.0,17.0,10.0,15.0] [30.0,5.0,34.0,12.0,14.0,16.0] [4.0,36.0,29.0,13.0,18.0,11.0] \u0026lt;class 'matlab.mlarray.double'\u0026gt;  Unfortunately, matlab package seems to work only with pure Python data structures, meaning that we will need to use some tricks if we are interested in working with, e.g., numpy arrays. This is important, as usually if we need to call a MATALB function to work on arrays, it is because in Python we were working with arrays and this is usually done via numpy.\nLet\u0026rsquo;s see what happens:\nimport numpy as np a = np.array([1,2,3,4]).reshape([1,4]) b = a**2 print(type(a)) print(type(b)) print(b) print(b.shape)  \u0026lt;type 'numpy.ndarray'\u0026gt; \u0026lt;type 'numpy.ndarray'\u0026gt; [[ 1 4 9 16]] (1, 4)  We created a numpy array a, and then we compute the square of each of its values, yelding another numpy array.\nIf we try to reproduce this operation using matlab package we will be stuck in an error as soon as we try to cast the numpy array a as a matlab.double array:\na_m = matlab.double(a)  --------------------------------------------------------------------------- ValueError Traceback (most recent call last) \u0026lt;ipython-input-10-1757930e4e37\u0026gt; in \u0026lt;module\u0026gt;() ----\u0026gt; 1 a_m = matlab.double(a) /media/DATA/miniconda3/envs/tomolab2/lib/python2.7/site-packages/matlab/mlarray.pyc in __init__(self, initializer, size, is_complex) 49 super(double, self).__init__('d', initializer, size, is_complex) 50 except Exception as ex: ---\u0026gt; 51 raise ex 52 53 ValueError: initializer must be a rectangular nested sequence  This happens because matlab.double function is expecting a list or a tuple as input, and it is unable to understand the numpy.ndarray datatype.\nA workaraound is to go back to the list format:\na_m = matlab.double(a.tolist()) # casting a as list b_m = eng.power(a_m,2.0) print((b_m)) print(type(b_m)) print(b_m.size)  [[1.0,4.0,9.0,16.0]] \u0026lt;class 'matlab.mlarray.double'\u0026gt; (1, 4)  There are additional problems that we need to face here: the output produced by the call to a MATLAB function is alway os type matlab.mlarray. This is usefull if it is the endpoint of our computation, but if we need to perform other operations (in Python) on the output of the MATLAB function, this format if of little to no use, for us.\nIf we want to be correct, matlab.mlarray is seen almost as a list in Python. Basic operations are supported, but even transpose or reshape throw errors. To overcome this limitation we can recast the output as nupmy array*.\nb_n = np.asarray(b_m) print(b_n) print(type(b_n)) print(b_n.shape)  [[ 1. 4. 9. 16.]] \u0026lt;type 'numpy.ndarray'\u0026gt; (1, 4)  This can be done also in one line of code:\na_m = matlab.double(a.tolist()) # casting a as list b_m = np.asarray(eng.power(a_m,2.0)) print((b_m)) print(type(b_m)) print(b_m.shape)  [[ 1. 4. 9. 16.]] \u0026lt;type 'numpy.ndarray'\u0026gt; (1, 4)  Calling custom MATLAB user scripts and functions from Python So far we have seen how we can use matlab.engine to call built-in MATLAB functions to perform some computation on data, and strategies to passa data from Python session to MATLAB workspace.\nThis is rearely something we are interested in. Often times, we will be looking for ways to run custom MATLAB code, which can be of different types:\n scripts (*.m) function (*.m) MEX function (*.mexa64)  Let\u0026rsquo;s start with a very basic example, and let\u0026rsquo;s assume that, again we want to compute the power of an array.\nWe can use the following MATLAB code:\nb = [1,2,3,4]; e = 2; r = b.^e  In your current folder, copy this MATLAB code in a file named pow_script.m.\nAfter you save the file, we can call it from within Python like this:\neng.pow_script(nargout=0)  r = 1 4 9 16  eng.workspace['r']  matlab.double([[1.0,4.0,9.0,16.0]])  Specifying nargout=0 is required. Although the script prints output, it returns no output arguments to Python.\nAlternatively (and in my opinion more interestingly) we can convert the script to a function and call the function from the engine.\nfunction r = pow_fun(b,e) r = b.^e; end  All the considerations previously made are still valid for a custom user function:\nbase = np.asarray([1.0,2.0,3.0,4.0]) exp = 2.0 ret = eng.pow_fun(matlab.double(base.tolist()),exp)  print(ret) print(type(ret))  [[1.0,4.0,9.0,16.0]] \u0026lt;class 'matlab.mlarray.double'\u0026gt;  And obviously this would allow us also to use complex MEX function within Python, passing Python arrays as input and receiving the output directly as Python variables (or numpy arrays).\n","date":1549358806,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549358806,"objectID":"337de16c82dbd4f4256c78faf27f73a4","permalink":"/post/matlab-from-python/","publishdate":"2019-02-05T10:26:46+01:00","relpermalink":"/post/matlab-from-python/","section":"post","summary":"Different strategies to call and use matlab scripts and functions from Python code.\n","tags":["matlab","matlab.engine","python"],"title":"Calling Matlab (custom) functions from Python","type":"post"},{"authors":["Michele Scipioni","Maria F Santarelli","Luigi Landini","Ciprian Catana","Douglas N Greve","Julie C Price","Stefano Pedemonte"],"categories":null,"content":"","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"77d290b6f390fab74e8fcf48c926fe43","permalink":"/publication/kinetic-compressive-sensing/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/publication/kinetic-compressive-sensing/","section":"publication","summary":"Parametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes (ICM) approach, alternating the optimization of activity time series (OS-MAP-OSL), and kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a simulated dynamic phantom: a bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial noise and better tissue contrast. A GPU-based open source implementation of the algorithm is provided.","tags":null,"title":"Kinetic Compressive Sensing","type":"publication"},{"authors":["Niccolo Fuin","Onofrio A Catalano","Michele Scipioni","Lisanne PW Canjels","David Izquierdo-Garcia","Stefano Pedemonte","Ciprian Catana"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"2b1d1e7feffc58efdcaabca4cd3e6e18","permalink":"/publication/concurrent-moco-fuin/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/concurrent-moco-fuin/","section":"publication","summary":"We present an approach for concurrent reconstruction of respiratory motion‚Äìcompensated abdominal dynamic contrast-enhanced (DCE)‚ÄìMRI and PET data in an integrated PET/MR scanner. The MR and PET reconstructions share the same motion vector fields derived from radial MR data; the approach is robust to changes in respiratory pattern and does not increase the total acquisition time. METHODS: PET and DCE-MRI data of 12 oncologic patients were simultaneously acquired for 6 min on an integrated PET/MR system after administration of 18F-FDG and gadoterate meglumine. Golden-angle radial MR data were continuously acquired simultaneously with PET data and sorted into multiple motion phases on the basis of a respiratory signal derived directly from the radial MR data. The resulting multidimensional dataset was reconstructed using a compressed sensing approach that exploits sparsity among respiratory phases. Motion vector fields obtained using the full 6-min (MC6-min) and only the last 1 min (MC1-min) of data were incorporated into the PET reconstruction to obtain motion-corrected PET images and in an MR iterative reconstruction algorithm to produce a series of motion-corrected DCE-MR images (moco_GRASP). The motion-correction methods (MC6-min and MC1-min) were evaluated by qualitative analysis of the MR images and quantitative analysis of SUVmax and SUVmean, contrast, signal-to-noise ratio (SNR), and lesion volume in the PET images. RESULTS: Motion-corrected MC6-min PET images demonstrated 30%, 23%, 34%, and 18% increases in average SUVmax, SUVmean, contrast, and SNR and an average 40% reduction in lesion volume with respect to the non‚Äìmotion-corrected PET images. The changes in these figures of merit were smaller but still substantial for the MC1-min protocol: 19%, 10%, 15%, and 9% increases in average SUVmax, SUVmean, contrast, and SNR; and a 28% reduction in lesion volume. Moco_GRASP images were deemed of acceptable or better diagnostic image quality with respect to conventional breath-hold Cartesian volumetric interpolated breath-hold examination acquisitions. CONCLUSIONS: We presented a method that allows the simultaneous acquisition of respiratory motion‚Äìcorrected diagnostic quality DCE-MRI and quantitatively accurate PET data in an integrated PET/MR scanner with negligible prolongation in acquisition time compared with routine PET/DCE-MRI protocols. ","tags":null,"title":"Concurrent Respiratory Motion Correction of Abdominal PET and Dynamic Contrast-Enhanced‚ÄìMRI Using a Compressed Sensing Approach","type":"publication"},{"authors":["Michele Scipioni","Stefano Pedemonte","Maria Filomena Santarelli","Luigi Landini"],"categories":null,"content":"","date":1535068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535068800,"objectID":"4b130b29f6dec5b82a2e657932e158a3","permalink":"/publication/pgm-pet_preprint/","publishdate":"2018-08-24T00:00:00Z","relpermalink":"/publication/pgm-pet_preprint/","section":"publication","summary":"In the context of dynamic emission tomography, the conventional processing pipeline consists of independent image reconstruction of single time frames, followed by the application of a suitable kinetic model to time activity curves (TACs) at the voxel or region-of-interest level. The relatively new field of 4D PET direct reconstruction, by contrast, seeks to move beyond this scheme and incorporate information from multiple time frames within the reconstruction task. Existing 4D direct models are based on a deterministic description of voxels' TACs, captured by the chosen kinetic model, considering the photon counting process the only source of uncertainty. In this work, we introduce a new probabilistic modeling strategy based on the key assumption that activity time course would be subject to uncertainty even if the parameters of the underlying dynamic process were known. This leads to a hierarchical Bayesian model, which we formulate using the formalism of Probabilistic Graphical Modeling (PGM). The inference of the joint probability density function arising from PGM is addressed using a new gradient-based iterative algorithm, which presents several advantages compared to existing direct methods: it is flexible to an arbitrary choice of linear and nonlinear kinetic model; it enables the inclusion of arbitrary (sub)differentiable priors for parametric maps; it is simpler to implement and suitable to integration in computing frameworks for machine learning. Computer simulations and an application to real patient scan showed how the proposed approach allows us to weight the importance of the kinetic model, providing a bridge between indirect and deterministic direct methods. ","tags":null,"title":"(preprint) Probabilistic Graphical Modeling approach to dynamic PET direct parametric map estimation and image reconstruction","type":"publication"},{"authors":["Onofrio A Catalano","Lale Umutlu","Niccolo Fuin","Matthew Louis Hibert","Michele Scipioni","Stefano Pedemonte","Mark Vangel","Andreea Maria Catana","Ken Herrmann","Felix Nensa","David Groshar","Umar Mahmood","Bruce R Rosen","Ciprian Catana"],"categories":null,"content":"","date":1531267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531267200,"objectID":"69d3f418a67fd29b37200781a6be584c","permalink":"/publication/comparison-moco-catalano/","publishdate":"2018-07-11T00:00:00Z","relpermalink":"/publication/comparison-moco-catalano/","section":"publication","summary":"PURPOSE: To compare the clinical performance of upper abdominal PET/DCE-MRI with and without concurrent respiratory motion correction (MoCo). METHODS: MoCo PET/DCE-MRI of the upper abdomen was acquired in 44 consecutive oncologic patients and compared with non-MoCo PET/MRI. SUVmax and MTV of FDG-avid upper abdominal malignant lesions were assessed on MoCo and non-MoCo PET images. Image quality was compared between MoCo DCE-MRI and non-MoCo CE-MRI, and between fused MoCo PET/MRI and fused non-MoCo PET/MRI images. RESULTS: MoCo PET resulted in higher SUVmax (10.8‚Äâ¬±‚Äâ5.45) than non-MoCo PET (9.62‚Äâ¬±‚Äâ5.42) and lower MTV (35.55‚Äâ¬±‚Äâ141.95 cm3) than non-MoCo PET (38.11‚Äâ¬±‚Äâ198.14 cm3; p‚Äâ","tags":null,"title":"Comparison of the clinical performance of upper abdominal PET/DCE-MRI with and without concurrent respiratory motion correction (MoCo)","type":"publication"},{"authors":["Michele Scipioni","Assuero Giorgetti","Daniele Della Latta","Sabrina Fucci","Vincenzo Positano","Luigi Landini","Maria F Santarelli"],"categories":null,"content":"","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"7dbfead78430909b288a5181df019560","permalink":"/publication/icm-em/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/publication/icm-em/","section":"publication","summary":"We propose and test a novel approach for direct parametric image reconstruction of dynamic PET data. We present a theoretical description of the problem of PET direct parametric maps estimation as an inference problem, from a probabilistic point of view, and we derive a simple iterative algorithm, based on the Iterated Conditional Mode (ICM) framework, which exploits the simplicity of a two-step optimization and the efficiency of an analytic method for estimating kinetic parameters from a nonlinear compartmental model. The resulting method is general enough to be flexible to an arbitrary choice of the kinetic model, and unlike many other solutions, it is capable to deal with nonlinear compartmental models without the need for linearization. We tested its performance on a two-tissue compartment model, including an analytical solution to the kinetic parameters evaluation, based on an auxiliary parameter set, with the aim of reducing computation errors and approximations. The new method is tested on simulated and clinical data. Simulation analysis led to the conclusion that the proposed algorithm gives a good estimation of the kinetic parameters in any noise condition. Furthermore, the application of the proposed method to clinical data gave promising results for further studies.","tags":null,"title":"Direct parametric maps estimation from dynamic PET data: an iterated conditional modes approach","type":"publication"},{"authors":["Michele Scipioni","Assuero Giorgetti","Daniele Della Latta","Sabrina Fucci","Vincenzo Positano","Luigi Landini","Maria F Santarelli"],"categories":null,"content":"","date":1529452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529452800,"objectID":"d58f3c9ececd17a766be43b8ba0a9501","permalink":"/publication/fast-kinetic-modeling/","publishdate":"2018-06-20T00:00:00Z","relpermalink":"/publication/fast-kinetic-modeling/","section":"publication","summary":"In this work, we propose and test a new approach for nonlinear kinetic parameters' estimation from dynamic PET data. A technique is discussed, to derive an analytical closed-form expression of the compartmental model used for kinetic parameters' evaluation, using an auxiliary parameter set, with the aim of reducing the computational burden and speeding up the fitting of these complex mathematical expressions to noisy TACs. Two alternative algorithms based on numeric calculations are considered and compared to the new proposal. We perform a simulation study aimed at (i) assessing agreement between the proposed method and other conventional ways of implementing compartmental model fitting, and (ii) quantifying the reduction in computational time required for convergence. It results in a speed-up factor of ~120 when compared to a fully numeric version, or ‚àº38, with respect to a more conventional implementation, while converging to very similar values for the estimated model parameters. The proposed method is also tested on dynamic 3D PET clinical data of four control subjects. The results obtained supported those of the simulation study, and provided input and promising perspectives for the application of the proposed technique in clinical practice.","tags":null,"title":"Accelerated PET kinetic maps estimation by analytic fitting method","type":"publication"},{"authors":null,"categories":null,"content":" Introducing a new set of functions able to deal with the common ¬µ-k parametrization of the Negative Binomial distribution for count data.\n[GitHub SOURCE CODE]\n Negative Binomial Distribution Negative binomial regression is for modeling count variables, usually for over-dispersed count outcome variables.\nNegative binomial regression Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean. It can be considered as a generalization of Poisson regression since it has the same mean structure as Poisson regression and it has an extra parameter to model the over-dispersion. If the conditional distribution of the outcome variable is over-dispersed, the confidence intervals for the Negative binomial regression are likely to be narrower as compared to those from a Poisson regression model.\nPoisson regression Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models.\nZero-inflated regression model Zero-inflated models attempt to account for excess zeros. In other words, two kinds of zeros are thought to exist in the data, ‚Äútrue zeros‚Äù and ‚Äúexcess zeros‚Äù. Zero-inflated models estimate two equations simultaneously, one for the count model and one for the excess zeros.\nOLS regression Count outcome variables are sometimes log-transformed and analyzed using OLS regression. Many issues arise with this approach, including loss of data due to undefined values generated by taking the log of zero (which is undefined), as well as the lack of capacity to model the dispersion.\nMatlab provides some functions to experiments with Negative Binomial Distribution.\nProblem is that, for this parcticular family of distribution, you can find different kind of parametrization. According to the problem you are trying to solve or reproduce, one parametrization can me better than another.\nFor a general idea of what I mean by different parametrization, you can have a look at the Wikipedia page related to NB distribution, at https://en.m.wikipedia.org/wiki/Negative_binomial_distribution#Alternative_formulations.\nMatlab choice of parametrization: r-p In its simplest form (when r is an integer), the negative binomial distribution models the number of failures x before a specified number of successes is reached in a series of independent, identical trials. Its parameters are the probability of success in a single trial, p, and the number of successes, r. A special case of the negative binomial distribution, when r = 1, is the geometric distribution, which models the number of failures before the first success.\nMore generally, r can take on non-integer values. This form of the negative binomial distribution has no interpretation in terms of repeated trials, but, like the Poisson distribution, it is useful in modeling count data. The negative binomial distribution is more general than the Poisson distribution because it has a variance that is greater than its mean, making it suitable for count data that do not meet the assumptions of the Poisson distribution. In the limit, as r increases to infinity, the negative binomial distribution approaches the Poisson distribution.\nTo deal with this version of negative binomial distribution, the Statistics and Machine Learning Toolbox provide the following set of functions:\n nbinrnd.m nbinlike.m nbinfit.m nbinpdf.m  \u0026ldquo;Ecological\u0026rdquo; parameterization of the negative binomial: ¬µ-k The ‚Äúecological‚Äù parameterization of the negative binomial replaces the parameters p (probability of success per trial) and n (number of successes before you stop counting failures) with ¬µ = n(1‚àíp)/p, the mean number of failures expected (or of counts in a sample), and k, which is typically called an overdispersion parameter.\nConfusingly, k is sometimes called size, because it is mathematically equivalent to n in the failure-process parameterization.\nThe overdispersion parameter measures the amount of clustering, or aggregation, or heterogeneity, in the data: a smaller k means more heterogeneity. The variance of the negative binomial distribution is ¬µ+¬µ^2/k, and so as k becomes large the variance approaches the mean and the distribution approaches the Poisson distribution. For k \u0026gt; 10, the negative binomial is hard to tell from a Poisson distribution, but k is often less than 1.\nSpecifically, you can get a negative binomial distribution as the result of a Poisson sampling process where the rate **Œª **itself varies. If the distribution of Œª is a gamma distribution with shape parameter k and mean ¬µ, and x is Poisson-distributed with mean Œª, then the distribution of x be a negative binomial distribution with mean ¬µ and overdispersion parameter k (May, 1978; Hilborn and Mangel, 1997). In this case, the negative binomial reflects unmeasured (‚Äúrandom‚Äù) variability in the population.\nWhile available in R, this kind of parametrization is not provided by Matlab most standard libraries, so this repo is about adding them so that you can have a choice of the best version of NB distribution you want to use.\nIn detail, the new versions of the Matlab files you can find here are the following:\n nbinrnd_mu.m nbinlike_mu.m nbinfit_mu.m nbinpdf_mu.m  ","date":1510826400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510826400,"objectID":"c3cef0c0178f6f9158102801f86c5a72","permalink":"/post/matlab_toolbox_nbin_mu/","publishdate":"2017-11-16T10:00:00Z","relpermalink":"/post/matlab_toolbox_nbin_mu/","section":"post","summary":"Introducing a new set of functions able to deal with the common ¬µ-k parametrization of the Negative Binomial distribution for count data.\n","tags":["MATLAB","software","toolbox","statistics","distribution","probability","Negative Binomial","NB"],"title":"Exending MATLAB's tools for Negative Binomial distributions: nbin*_mu.m","type":"post"},{"authors":null,"categories":null,"content":"","date":1510790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510790400,"objectID":"1db1e99ce9b60c612857439d4eba9062","permalink":"/project/matlab_nbin_mu/","publishdate":"2017-11-16T00:00:00Z","relpermalink":"/project/matlab_nbin_mu/","section":"project","summary":"This repo is about adding the missing ¬µ-k parametrization for the Negative Binomial (NB) distribution in Matlab.","tags":["Matlab","software","toolbox","statistics","distribution","probability","Negative Binomial","NB"],"title":"MATLAB toolbox: nbin*_mu","type":"project"},{"authors":null,"categories":null,"content":"  Fixed some issues on code duplication\n  Created a setup file to add all the needed source files to Matlab\u0026rsquo;s path   Added some new colormap and a multiple choice of how to set up colorbar limit to improve the quality of image visualization   Added a new example dataset with heart data   ","date":1510740000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510740000,"objectID":"48c6d78dcad2fe329b3a4946d01f0e28","permalink":"/post/update_kmtool_repo_14november17/","publishdate":"2017-11-15T11:00:00+01:00","relpermalink":"/post/update_kmtool_repo_14november17/","section":"post","summary":"Kinetic Modeling Toolbox designed to estimate kinetic parameters from 4D PET and DCE-MRI dataset at a ROI level.","tags":["PET","DCE-MRI","emission tomography","contrast enhanced mri","kinetic modeling","compartmental models","Matlab"],"title":"Changelog for KMtoolbox repository - November 14, 2017","type":"post"},{"authors":null,"categories":null,"content":"New GitHub repository with material from given lectures (Update Nov 2017) During my Ph.D. at the University of Pisa, I have had the chance to work as a Graduate Teaching Assistant for the following classes:\n2017  [Ing-Inf/06]: Biomedical Imaging (6 CFU) (ITA)   Iterative reconstruction techniques for emission tomography imaging: ML-EM, OS-EM, and MAP-OSL-EM [Download slides]** [Lecture notes]**    2016  [Ing-Inf/06]: Biomedical Imaging (6 CFU) (ITA)   Iterative reconstruction techniques for emission tomography imaging: ML-EM, OS-EM, and MAP-OSL-EM **[Download slides]**  Introduction to kinetic modeling for emission tomography: focus on compartmental models (meaning, use, and interpratation) [Part1]** [Part2]**    ","date":1510663774,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510663774,"objectID":"649f216d97878e0cda656698b44e0504","permalink":"/post/updated-lectures-repository/","publishdate":"2017-11-14T13:49:34+01:00","relpermalink":"/post/updated-lectures-repository/","section":"post","summary":"Introduction of the new repository and list of lectures material organized by year.","tags":["lecture","university","medical imaging","PET"],"title":"Creation of a GitHub repository for lectures material","type":"post"},{"authors":null,"categories":null,"content":"Introduction\nParametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters.\nMethods\nThe model has three key components: the model of the acquisition system; the kinetic model; and a Markov Random Field with an L1-norm cost function, defined in kinetic parameters domain. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes approach, alternating the optimization of activity time series (OSL-MAP-EM), and kinetic parameters (MAP-LM): a parallel GPU implementation allows synchronized update of all the voxels, computing the gradient of the log joint posterior at each iteration.\nExperiments\n100 noise realizations of a simulated dynamic geometric phantom were generated using a 2TC irreversible model. A bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial and better tissue contrast.\nConclusion\nCombining sparse kinetic compressive sensing into a direct reconstruction framework can help generating high-quality images and parametric maps, both amenable for display and quantitatively more accurate than what a post-reconstruction fitting can achieve. A GPU-based open source implementation of the algorithm is provided.\n","date":1508976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508976000,"objectID":"dffd7b7bcc19a5cafc2b73175a98def9","permalink":"/talk/ieee-nss-mic-2017/","publishdate":"2017-10-26T00:00:00Z","relpermalink":"/talk/ieee-nss-mic-2017/","section":"talk","summary":"Introduction\nParametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting.","tags":null,"title":"Kinetic Compressive Sensing","type":"talk"},{"authors":null,"categories":null,"content":"The source code and detailed instruction about how to install and use this toolbox will be provided soon. Keep checking the website for future updates.\n","date":1498435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498435200,"objectID":"a4b3df7c10bebbc4a7257f63262d06c7","permalink":"/project/gpu-lmfit/","publishdate":"2017-06-26T00:00:00Z","relpermalink":"/project/gpu-lmfit/","section":"project","summary":"GPU-LMfit - Python/CUDA library for parallel fitting of compartmental models to 4D medical imaging volumes.","tags":["PET","DCE-MRI","emission tomography","contrast enhanced mri","kinetic modeling","compartmental models","cuda","Nvidia"],"title":"CUDA-GPU kinetic modeling","type":"project"},{"authors":null,"categories":null,"content":" OFFICIAL WEBSITE OF THE PROJECT\nOcciput.io is an open source software for Tomographic reconstruction based on GPU computing and on Python.\nThe design of occiput.io makes it extremely easy to reconstruct tomographic images starting from the raw data produced by imaging systems: PET, PET-MRI and SPECT. Occiput.io is designed for GPU computing, it‚Äôs blazing fast!\nTo date, Occiput and the NiftyRec ray-tracer (on which Occiput is based), have been downloaded more than 12000 times.\nThe design of occiput.io enables 2D, 3D (volumetric) and 4D (spatio-temporal) dynamic tomographic imaging, joint reconstruction of multiple parameters (e.g. MLAA), motion-aware imaging and more.\nOcciput enables the interactive tomographic reconstruction in the cloud, using Jupyter and IPython.\nA Python package implementing the interface to the Siemens Biograph mMR PET-MRI scanner (including access to raw listmode data, sinograms, scatter data, physiological data) is available upon request (occiput.reconstruction@gmail.com). Authorization from Siemens will be required.\nTo get started with Occiput, go to the project source code and follow the installation instructions. The source code contains Jupyter notebooks with documentation and examples.\n","date":1498435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498435200,"objectID":"ff5ece861bc8c6dd4abe8d4d38d4201c","permalink":"/project/occiput/","publishdate":"2017-06-26T00:00:00Z","relpermalink":"/project/occiput/","section":"project","summary":"Open source tomographic reconstruction software for 2D, 3D and 4D PET, PET-MRI and SPECT, in Python using GPUs.","tags":["PET","SPECT","emission tomography","transmission tomography","tomographic reconstruction","nuclear magnetic resonance"],"title":"Occiput.io","type":"project"},{"authors":null,"categories":null,"content":"Introduction\nParametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters.\nMethods\nThe model has three key components: the model of the acquisition system; the kinetic model; and a Markov Random Field with an L1-norm cost function, defined in kinetic parameters domain. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes approach, alternating the optimization of activity time series (OSL-MAP-EM), and kinetic parameters (MAP-LM): a parallel GPU implementation allows synchronized update of all the voxels, computing the gradient of the log joint posterior at each iteration.\nExperiments\n100 noise realizations of a simulated dynamic geometric phantom were generated using a 2TC irreversible model. A bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial and better tissue contrast.\nConclusion\nCombining sparse kinetic compressive sensing into a direct reconstruction framework can help generating high-quality images and parametric maps, both amenable for display and quantitatively more accurate than what a post-reconstruction fitting can achieve. A GPU-based open source implementation of the algorithm is provided.\nKeyword- parametric images,PET,compartmental models,compressive sensing,hierarchical Bayesian model,sparsity,Markov Random Field,FDG,GPU\n","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495670400,"objectID":"3cbb3467000fb011862b87ccb4ad558a","permalink":"/talk/martinosopenhouse17/","publishdate":"2017-05-25T00:00:00Z","relpermalink":"/talk/martinosopenhouse17/","section":"talk","summary":"Introduction\nParametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting.","tags":null,"title":"Kinetic compressive sensing: improving image reconstruction and parametric maps","type":"talk"},{"authors":["MF Santarelli","N Vanello","M Scipioni","G Valvano","L Landini"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"4fc3506e2638aad036c583d5531a2ca7","permalink":"/publication/newimagingfrontiers/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/publication/newimagingfrontiers/","section":"publication","summary":"**Background**: Among the novelties in the field of cardiovascular imaging, the construction of quantitative maps in a fast and efficient way is one of the most interesting aspects of the clinical research. Quantitative parametric maps are typically obtained by post processing dynamic images, that is, sets of images usually acquired in different temporal intervals, where several images with different contrasts are obtained. Magnetic resonance (MR) imaging, and emission tomography (positron emission and single photon emission) are the imaging techniques best suited for the formation of quantitative maps. **Methods**: In this review article we present several methods that can be used for obtaining parametric maps, in a fast way, starting from the acquired raw data. We describe both methods commonly used in clinical research, and more innovative methods that build maps directly from the raw data, without going through the image reconstruction. **Results**: We briefly described recently developed methods in magnetic resonance (MR) imaging that accelerate further the MR raw data generation, based on appropriate sub-sampling of k-space; then, we described recently developed methods for generating MR parametric maps. With regard to the emission tomography techniques, we gave an overview of both conventional methods, and more recently developed direct estimation algorithms for parametric image reconstruction from dynamic positron emission tomography data. **Conclusion**: We have provided an overview of the possible approaches that can be followed to realize useful parametric maps from imaging raw data. We moved from the conventional approaches to more recent and efficient methods for accelerating the raw data generation and the of parametric maps formation.","tags":null,"title":"New Imaging Frontiers in Cardiology: Fast and Quantitative Maps from Raw Data","type":"publication"},{"authors":null,"categories":null,"content":"Kinetic Modeling Toolbox designed to estimate kinetic parameters from 4D PET and DCE-MRI dataset at a ROI level\n[GitHub SOURCE CODE]\n Updates: November 14, 2017 \u0026ndash;\u0026gt; [Check the related blog post]\n\u0026gt; Fixed some issues on code duplication \u0026gt; Created a setup file to add all the needed source files to Matlab's path \u0026gt; Added some new colormap and a multiple choice of how to set up colorbar limit to improve the quality of image visualization \u0026gt; Added a new example dataset with heart data  June, 2017\n\u0026gt; First commit and publication of the toolbox on GitHub   A few screenshots: Loading 4D volume:\nSelect colormap and adjust visual scale:\nMain window after loading 4D volume:\nRoi selection mode (example: selecting input function):\nFitting image-derived AIF with a theoretical model:\nAfter selecting a tissue ROI, choose the suitable model:\nFitting result:\nResiduals:\n","date":1473638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473638400,"objectID":"e310e9dcdf730ef62363bb7db82c7c9c","permalink":"/project/kmtool/","publishdate":"2016-09-12T00:00:00Z","relpermalink":"/project/kmtool/","section":"project","summary":"Kinetic Modeling Toolbox designed to estimate kinetic parameters from 4D PET and DCE-MRI dataset at a ROI level.","tags":["PET","DCE-MRI","emission tomography","contrast enhanced mri","kinetic modeling","compartmental models","Matlab"],"title":"KMtool: Kinetic Modeling Toolbox","type":"project"},{"authors":["MF Santarelli","D Della Latta","M Scipioni","V Positano","L Landini"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"a0c81888f1bb76cd247d554acfc55a2d","permalink":"/publication/com-poisson/","publishdate":"2016-08-01T00:00:00Z","relpermalink":"/publication/com-poisson/","section":"publication","summary":"Positron emission tomography (PET) in medicine exploits the properties of positron-emitting unstable nuclei. The pairs of Œ≥- rays emitted after annihilation are revealed by coincidence detectors and stored as projections in a sinogram. It is well known that radioactive decay follows a Poisson distribution; however, deviation from Poisson statistics occurs on PET projection data prior to reconstruction due to physical effects, measurement errors, correction of deadtime, scatter, and random coincidences. A model that describes the statistical behavior of measured and corrected PET data can aid in understanding the statistical nature of the data: it is a prerequisite to develop efficient reconstruction and processing methods and to reduce noise. The deviation from Poisson statistics in PET data could be described by the Conway-Maxwell-Poisson (CMP) distribution model, which is characterized by the centring parameter Œª and the dispersion parameter ŒΩ, the latter quantifying the deviation from a Poisson distribution model. In particular, the parameter ŒΩ allows quantifying over-dispersion (ŒΩ1) of data. A simple and efficient method for Œª and ŒΩ parameters estimation is introduced and assessed using Monte Carlo simulation for a wide range of activity values. The application of the method to simulated and experimental PET phantom data demonstrated that the CMP distribution parameters could detect deviation from the Poisson distribution both in raw and corrected PET data. It may be usefully implemented in image reconstruction algorithms and quantitative PET data analysis, especially in low counting emission data, as in dynamic PET data, where the method demonstrated the best accuracy.","tags":null,"title":"A Conway-Maxwell-Poisson (CMP) model to address data dispersion on positron emission tomography","type":"publication"},{"authors":null,"categories":null,"content":"Strongly inspired by an article by Kevin Hughes (https://github.com/kevinhughes27?tab=repositories) Today I am going to show you how we can do Blind Source Separation (BSS) using algorithms available in the Shogun Machine Learning Toolbox. What is Blind Source Separation? BSS is the separation of a set of source signals from a set of mixed signals.\nMy favorite example of this problem is known as the cocktail party problem where a number of people are talking simultaneously and we want to separate each persons speech so we can listen to it separately. Now the caveat with this type of approach is that we need as many mixtures as we have source signals or in terms of the cocktail party problem we need as many microphones as people talking in the room.\nLet\u0026rsquo;s get started. This example is going to be in Python and the first thing we are going to need to do is load some audio files. To make things a bit easier further on in this example I\u0026rsquo;m going to wrap the basic scipy wav file reader and add some additional functionality. First I added a case to handle converting stereo wav files back into mono wav files and secondly this loader takes a desired sample rate and resamples the input to match. This is important because when we mix the two audio signals they need to have the same sample rate. from scipy.io import wavfile from scipy.signal import resample def load_wav(filename,samplerate=44100): # load file rate, data = wavfile.read(filename) # convert stereo to mono if len(data.shape) \u0026gt; 1: data = data[:,0]/2 + data[:,1]/2 # re-interpolate samplerate ratio = float(samplerate) / float(rate) data = resample(data, len(data) * ratio) return samplerate, data.astype(np.int16)  Next we\u0026rsquo;re going to need a way to play the audio files we\u0026rsquo;re working with (otherwise this wouldn\u0026rsquo;t be very exciting at all would it?). In the next bit of code I\u0026rsquo;ve defined a wavPlayer class that takes the signal and the sample rate and then creates a nice HTML5 webplayer right inline with the notebook.\n#import StringIO import base64 import struct from IPython.core.display import HTML def wavPlayer(data, rate): \u0026quot;\u0026quot;\u0026quot; will display html 5 player for compatible browser The browser need to know how to play wav through html5. there is no autoplay to prevent file playing when the browser opens Adapted from SciPy.io. and github.com/Carreau/posts/blob/master/07-the-sound-of-hydrogen.ipynb \u0026quot;\u0026quot;\u0026quot; buffer = six.moves.StringIO() buffer.write(b'RIFF') buffer.write(b'\\x00\\x00\\x00\\x00') buffer.write(b'WAVE') buffer.write(b'fmt ') if data.ndim == 1: noc = 1 else: noc = data.shape[1] bits = data.dtype.itemsize * 8 sbytes = rate*(bits // 8)*noc ba = noc * (bits // 8) buffer.write(struct.pack('\u0026lt;ihHIIHH', 16, 1, noc, rate, sbytes, ba, bits)) # data chunk buffer.write(b'data') buffer.write(struct.pack('\u0026lt;i', data.nbytes)) if data.dtype.byteorder == '\u0026gt;' or (data.dtype.byteorder == '=' and sys.byteorder == 'big'): data = data.byteswap() buffer.write(data.tostring()) # return buffer.getvalue() # Determine file size and place it in correct # position at start of the file. size = buffer.tell() buffer.seek(4) buffer.write(struct.pack('\u0026lt;i', size-8)) val = buffer.getvalue() src = \u0026quot;\u0026quot;\u0026quot; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Simple Test\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;audio controls=\u0026quot;controls\u0026quot; style=\u0026quot;width:600px\u0026quot; \u0026gt; \u0026lt;source controls src=\u0026quot;data:audio/wav;base64,{base64}\u0026quot; type=\u0026quot;audio/wav\u0026quot; /\u0026gt; Your browser does not support the audio element. \u0026lt;/audio\u0026gt; \u0026lt;/body\u0026gt; \u0026quot;\u0026quot;\u0026quot;.format(base64=base64.encodestring(val)) display(HTML(src))  Now that we can load and play wav files we actually need some wav files! I found the sounds from Starcraft to be a great source of wav files because they\u0026rsquo;re short, interesting and remind me of my childhood. You can download Starcraft wav files here: http://wavs.unclebubby.com/computer/starcraft/ among other places on the web or from your Starcraft install directory (come on I know it\u0026rsquo;s still there).\nAnother good source of data (although lets be honest less cool) is ICA central and various other more academic data sets: http://perso.telecom-paristech.fr/~cardoso/icacentral/base_multi.html. Note that for lots of these data sets the data will be mixed already so you\u0026rsquo;ll be able to skip the next few steps.\nOkay lets load up an audio file. I chose the Terran Battlecruiser saying \u0026ldquo;Good Day Commander\u0026rdquo;. In addition to the creating a wavPlayer I also plotted the data using Matplotlib (and tried my best to have the graph length match the HTML player length). Have a listen!\n# change to the shogun-data directoy import os os.chdir('../files')  %pylab inline import pylab as pl import numpy as np # load fs1,s1 = load_wav('audio1.wav') # Terran Marine - \u0026quot;You want a piece of me, boy?\u0026quot; # plot pl.figure(figsize=(7,2)) pl.plot(s1) pl.title('Signal 1') pl.show() # player wavPlayer(s1, fs1)    \n Now let\u0026rsquo;s load a second audio clip:\n# load fs2,s2 = load_wav('audio2.wav') # Terran Battlecruiser - \u0026quot;Good day, commander.\u0026quot; # plot pl.figure(figsize=(6.75,2)) pl.plot(s2) pl.title('Signal 2') pl.show() # player wavPlayer(s2, fs2)    \n and a third audio clip:\n# load fs3,s3 = load_wav('audio3.wav') # Protoss Zealot - \u0026quot;My life for Aiur!\u0026quot; # plot pl.figure(figsize=(6.75,2)) pl.plot(s3) pl.title('Signal 3') pl.show() # player wavPlayer(s3, fs3)    \n Now we\u0026rsquo;ve got our audio files loaded up into our example program. The next thing we need to do is mix them together!\nFirst another nuance - what if the audio clips aren\u0026rsquo;t the same lenth? The solution I came up with for this was to simply resize them all to the length of the longest signal, the extra length will just be filled with zeros so it won\u0026rsquo;t affect the sound.\nThe signals are mixed by creating a mixing matrix $A$ and taking the dot product of $A$ with the signals $S$.\nAfterwards I plot the mixed signals and create the wavPlayers, have a listen!\n# Adjust for different clip lengths fs = fs1 length = max([len(s1), len(s2), len(s3)]) s1.resize((length,1), refcheck=False) s2.resize((length,1), refcheck=False) s3.resize((length,1), refcheck=False) \u0026quot;\u0026quot;\u0026quot; The function numpy.c_ concatenates the numpy arrays given as input. The method numpy_array.T is the transpose operation that allow us to prepare an input source matrix of the right size (3, length), according to the chosen mixing matrix (3,3). \u0026quot;\u0026quot;\u0026quot; S = (np.c_[s1, s2, s3]).T # Mixing Matrix #A = np.random.uniform(size=(3,3)) #A = A / A.sum(axis=0) A = np.array([[1, 0.5, 0.5], [0.5, 1, 0.5], [0.5, 0.5, 1]]) print 'Mixing Matrix:' print A.round(2) # Mixed Signals X = np.dot(A,S) # Exploring Mixed Signals for i in range(X.shape[0]): pl.figure(figsize=(6.75,2)) pl.plot((X[i]).astype(np.int16)) pl.title('Mixed Signal %d' % (i+1)) pl.show() wavPlayer((X[i]).astype(np.int16), fs)  Mixing Matrix: [[ 1. 0.5 0.5] [ 0.5 1. 0.5] [ 0.5 0.5 1. ]]    \n   \n   \n Now before we can work on separating these signals we need to get the data ready for Shogun. Thankfully this is pretty easy!\nfrom shogun.Features import RealFeatures # Convert to features for shogun mixed_signals = RealFeatures((X).astype(np.float64))  Now lets unmix those signals!\nIn this example I\u0026rsquo;m going to use an Independent Component Analysis (ICA) algorithm called JADE. JADE is one of the ICA algorithms available in Shogun and it works by performing Approximate Joint Diagonalization (AJD) on a 4th order cumulant tensor. I\u0026rsquo;m not going to go into a lot of detail on how JADE works behind the scenes but here is the reference for the original paper:\nCardoso, J. F., \u0026amp; Souloumiac, A. (1993). Blind beamforming for non-Gaussian signals. In IEEE Proceedings F (Radar and Signal Processing) (Vol. 140, No. 6, pp. 362-370). IET Digital Library.\n Note:\n  Shogun also has several other ICA algorithms including the Second Order Blind Identification (SOBI) algorithm, FFSep, JediSep, UWedgeSep and FastICA. All of the algorithms inherit from the ICAConverter base class and share some common methods for setting an intial guess for the mixing matrix, retrieving the final mixing matrix and getting/setting the number of iterations to run and the desired convergence tolerance. Some of the algorithms have additional getters for intermediate calculations, for example Jade has a method for returning the 4th order cumulant tensor while the \u0026ldquo;Sep\u0026rdquo; algorithms have a getter for the time lagged covariance matrices. Check out the source code on GitHub or the Shogun docs for more details!\n from shogun.Converter import Jade # Separating with JADE jade = Jade() signals = jade.apply(mixed_signals) S_ = signals.get_feature_matrix() A_ = jade.get_mixing_matrix() A_ = A_ / A_.sum(axis=0) print 'Estimated Mixing Matrix:' print A_  Estimated Mixing Matrix: [[ 0.25098835 0.49907993 0.24442146] [ 0.26235007 0.25543257 0.53186567] [ 0.48666158 0.2454875 0.22371287]]  Thats all there is to it!\nCheck out how nicely those signals have been separated and have a listen!\n# Show separation results # Separated Signal i gain = 4000 for i in range(S_.shape[0]): pl.figure(figsize=(6.75,2)) pl.plot((gain*S_[i]).astype(np.int16)) pl.title('Separated Signal %d' % (i+1)) pl.show() wavPlayer((gain*S_[i]).astype(np.int16), fs)    \n   \n   \n BSS isn\u0026rsquo;t only useful for working with Audio, it is also useful for image processing and pre-processing other forms of high dimensional data. Have a google for ICA and machine learning if you want to learn more, but we will sure come back in the future on this topic!\n","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"7030be2516be9b13769751c91d8b9270","permalink":"/post/bss-shogun-python/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/bss-shogun-python/","section":"post","summary":"How to do Blind Source Separation (BSS) using algorithms available in the Shogun Machine Learning Toolbox.\n","tags":["dicom","medical imaging","python"],"title":"Blind Source Separation (BSS) with the Shogun Machine Learning Toolbox","type":"post"},{"authors":null,"categories":null,"content":"The emcee() python module emcee can be used to obtain the posterior probability distribution of parameters, given a set of experimental data. An example problem is a double exponential decay. A small amount of Gaussian noise is also added.%matplotlib inline import numpy as np import lmfit from matplotlib import pyplot as plt import corner import emcee from pylab import * ion()  x = np.linspace(1, 10, 250) np.random.seed(0) y = 3.0 * np.exp(-x / 2) - 5.0 * np.exp(-(x - 0.1) / 10.) + 0.1 * np.random.randn(len(x)) plt.plot(x, y)  [\u0026lt;matplotlib.lines.Line2D at 0x7fbabac52310\u0026gt;]  Initializing our example creating a parameter set for the initial guesses: p = lmfit.Parameters() p.add_many(('a1', 4.), ('a2', 4.), ('t1', 3.), ('t2', 3., True)) def residual(p): v = p.valuesdict() return v['a1'] * np.exp(-x / v['t1']) + v['a2'] * np.exp(-(x - 0.1) / v['t2']) - y  Solving with minimize() gives the Maximum Likelihood solution.: mi = lmfit.minimize(residual, p, method='Nelder') #mi = lmfit.minimize(residual, p) lmfit.printfuncs.report_fit(mi.params, min_correl=0.5) plt.plot(x, y) plt.plot(x, residual(mi.params) + y, 'r') plt.show()  [[Variables]] a1: 2.98623688 (init= 4) a2: -4.33525596 (init= 4) t1: 1.30993185 (init= 3) t2: 11.8240752 (init= 3) [[Correlations]] (unreported correlations are \u0026lt; 0.500)  However, this doesn‚Äôt give a probability distribution for the parameters. Furthermore, we wish to deal with the data uncertainty. This is called marginalisation of a nuisance parameter. emcee requires a function that returns the log-posterior probability.\nPosterior distribution estimation The log-posterior probability is a sum of the log-prior probability and log-likelihood functions. The log-prior probability is assumed to be zero if all the parameters are within their bounds and -np.inf if any of the parameters are outside their bounds.:\n# add a noise parameter mi.params.add('f', value=1, min=0.001, max=2) # This is the log-likelihood probability for the sampling. We're going to estimate the # size of the uncertainties on the data as well. def lnprob(p): resid = residual(p) s = p['f'] resid *= 1 / s resid *= resid resid += np.log(2 * np.pi * s**2) return -0.5 * np.sum(resid)  Lets have a look at those posterior distributions for the parameters.\nmini = lmfit.Minimizer(lnprob, mi.params) res = mini.emcee(burn=300, steps=600, thin=3, params=mi.params) corner.corner(res.flatchain, labels=res.var_names, truths=list(res.params.valuesdict().values()))  The values reported in the MinimizerResult are the medians of the probability distributions and a 1 sigma quantile, estimated as half the difference between the 15.8 and 84.2 percentiles.\nThe median value is not necessarily the same as the Maximum Likelihood Estimate. We‚Äôll get that as well. You can see that we recovered the right uncertainty level on the data:\nlmfit.report_fit(mi.params) print('---------------------------------------------') print(\u0026quot;median of posterior probability distribution\u0026quot;) print('---------------------------------------------') lmfit.report_fit(res.params)  [[Variables]] a1: 2.98623688 (init= 4) a2: -4.33525596 (init= 4) t1: 1.30993185 (init= 3) t2: 11.8240752 (init= 3) f: 1 (init= 1) [[Correlations]] (unreported correlations are \u0026lt; 0.100) --------------------------------------------- median of posterior probability distribution --------------------------------------------- [[Variables]] a1: 2.99754553 +/- 0.151322 (5.05%) (init= 2.986237) a2: -4.33867001 +/- 0.117687 (2.71%) (init=-4.335256) t1: 1.31237613 +/- 0.132677 (10.11%) (init= 1.309932) t2: 11.8062444 +/- 0.457356 (3.87%) (init= 11.82408) f: 0.09810770 +/- 0.004350 (4.43%) (init= 1) [[Correlations]] (unreported correlations are \u0026lt; 0.100) C(a2, t2) = 0.980 C(a2, t1) = -0.926 C(t1, t2) = -0.873 C(a1, t1) = -0.541 C(a1, a2) = 0.224 C(a1, t2) = 0.171  Let\u0026rsquo;s find the maximum likelihood solution highest_prob = np.argmax(res.lnprob) hp_loc = np.unravel_index(highest_prob, res.lnprob.shape) mle_soln = res.chain[hp_loc] for i, par in enumerate(p): p[par].value = mle_soln[i] print(\u0026quot;\\nMaximum likelihood Estimation\u0026quot;) print('-----------------------------') print(p)  Maximum likelihood Estimation ----------------------------- Parameters([('a1', \u0026lt;Parameter 'a1', 2.9874185587879265, bounds=[-inf:inf]\u0026gt;), ('a2', \u0026lt;Parameter 'a2', -4.3357546840836836, bounds=[-inf:inf]\u0026gt;), ('t1', \u0026lt;Parameter 't1', 1.3090319527167826, bounds=[-inf:inf]\u0026gt;), ('t2', \u0026lt;Parameter 't2', 11.823518108067935, bounds=[-inf:inf]\u0026gt;)])  Finally lets work out a 1 and 2-sigma error estimate for \u0026lsquo;t1\u0026rsquo; quantiles = np.percentile(res.flatchain['t1'], [2.28, 15.9, 50, 84.2, 97.7]) print(\u0026quot;2 sigma spread\u0026quot;, 0.5 * (quantiles[-1] - quantiles[0]))  ('2 sigma spread', 0.2826990333440581)  ","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"490c095341829b5d01097582ae8f29f3","permalink":"/post/posterior-distribution-of-parameter-estimate/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/posterior-distribution-of-parameter-estimate/","section":"post","summary":"Exploring the functionality of the ***emcee*** and ***pymc*** Python modules.\n","tags":["bayes","bayesian","emcee","posterior","pymc","python"],"title":"Calculating the posterior probability distribution of parameters with emcee python module","type":"post"},{"authors":null,"categories":null,"content":"There are several data fitting utilities available. We will focus on two:\n  scipy.optimize\n  lmfit.minimize\n  Using both those modules, you can fit any arbitrary function that you define and it is, also, possible to constrain given parameters during the fit. Another important aspect is that both packages come with useful diagnostic tools.\nFitting Basics The fitting we discuss here is an iterative process.\n  First, we define our desired function, and calculate values given certain parameters\n  Then we calculate the difference between the initial and the new values\n  The final aim is to minimize this difference (specifically, we generally minimize the sum of the squares of these differences).Several examples can be found at http://www.scipy.org/Cookbook/FittingData\nMinimization is usually done by the method of least squares fitting. There are several algorithms available for this minimization.\n  The most common is the Levenberg-Marquardt:\n Susceptible to finding local minima instead of global Fast Usually well-behaved for most functions By far the most tested of methods, with many accompanying statistics implemented    Other methods include the Nelder-Mead, L-BFGS-B, and Simulated Annealing algorithms\n  Goodness-of-Fit (GoF) There are several statistics that can help you determine the goodness-of-fit. Most commonly used are:\n reduced chi-squared Standard error  You can get these and other tools for free with lmfit.minimize\nExample 1: Fit a quadratic curve with no constraints First, let\u0026rsquo;s try fitting a simple quadratic to some fake data:\n$$ y = ax^2 + bx + c $$\nWhat we will do:\n Generate some data for the example Define the function we wish to fit Use scipy.optimize to do the actual optimization  Let\u0026rsquo;s assume the following:\n The x-data is an array from -3 to 10 The y-data is $x^2$, with some random noise added. Let\u0026rsquo;s put our initial guesses for the coefficients a,b,c into a list called p0 (for fit parameters)  import numpy as np #Generate the arrays xarray1=np.arange(-3,10,.2) yarray1=xarray1**2 #Adding noise yarray1+=np.random.randn(yarray1.shape[0])*2 p0=[2,2,2] #Our initial guesses for our fit parameters  Since we are dealing with a quadratic fit we can use a cheap \u0026amp; easy method for polynomials (only): scipy.polyfit()\nThis method involves the least amount of setup while it simply outputs an array of the coefficients that best fit the data to the specified polynomial order.\n%matplotlib inline from scipy import polyfit from scipy.optimize import leastsq as lsq import matplotlib.pyplot as plt # polyfit(x, y, deg) fitcoeffs=polyfit(xarray1,yarray1,2) print \u0026quot;Parameter fitted using polyfit\u0026quot; print fitcoeffs  Parameter fitted using polyfit [ 1.00811611 -0.21729382 0.6272779 ]  Define the function you want to fit, remembering that p will be our array of initial guesses to the fit parameters, the coefficients a, b, c:\ndef quadratic(p,x): y_out=p[0]*(x**2)+p[1]*x+p[2] return y_out #Is the same as #quadratic = lambda p,x: p[0]*(x**2)+p[1]*x+p[2]  Then we define a function that returns the difference between the fit iteration value and the initial data:\nquadraticerr = lambda p,x,y: quadratic(p,x)-y  This difference or residual is the quantity that we will minimize with scipy.optimize. To do so, we call the least-squares optimization routine with scipy.optimize.leastsq() that stores the parameters you fit in the zeroth element of the output:\nfitout=lsq(quadraticerr,p0[:],args=(xarray1,yarray1)) paramsout=fitout[0] #These are the fitted coefficients covar=fitout[1] #This is the covariance matrix output print('Fitted Parameters using scipy\\'s leastsq():\\na = %.2f , b = %.2f , c = %.2f' % (paramsout[0],paramsout[1],paramsout[2]))  Fitted Parameters using scipy's leastsq(): a = 1.01 , b = -0.22 , c = 0.63  Now to get an array values for the results, just call your function definition with the fitted parameters, while the residuals, of course, will just be their difference from the original data:\nfitarray1=quadratic(paramsout,xarray1) residualarray1=fitarray1-yarray1 plt.rc('font',family='serif') fig1=plt.figure(1) frame1=fig1.add_axes((.1,.3,.8,.6)) #xstart, ystart, xwidth, yheight --\u0026gt; units are fraction of the image from bottom left xsmooth=np.linspace(xarray1[0],xarray1[-1]) plt.plot(xarray1,yarray1,'.') plt.plot(xsmooth,quadratic(paramsout,xsmooth)) frame1.set_xticklabels([]) #We will plot the residuals below, so no x-ticks on this plot plt.title('Quadratic Fit Example') plt.ylabel('y-data') plt.grid(True) frame1.annotate('$y$ = %.2f$\\cdot x^2$+%.2f$\\cdot x$+%.2f'%(paramsout[0],paramsout[1],paramsout[2]), \\ xy=(.05,.95),xycoords='axes fraction',ha=\u0026quot;left\u0026quot;,va=\u0026quot;top\u0026quot;,bbox=dict(boxstyle=\u0026quot;round\u0026quot;, fc='1')) from matplotlib.ticker import MaxNLocator plt.gca().yaxis.set_major_locator(MaxNLocator(prune='lower')) #Removes lowest ytick label frame2=fig1.add_axes((.1,.1,.8,.2)) plt.plot(xarray1,quadratic(paramsout,xarray1)-yarray1) plt.ylabel('Residuals') plt.grid(True) plt.show()  Example 2: More complex functions, with constraints Often we want to set limits on the values that our fitted parameters can have, for example, to be sure that one of the parameters can\u0026rsquo;t be negative, etc.\nTo do this, we can use scipy.optimize.minimize() or another useful package could be lmfit.minimize():\n We create an lmfit.Parameters() object We can set limits for the parameters to be fit We can even tell some params not to vary at all  The Parameters() object is then updated with every iteration.\nLet\u0026rsquo;s use more real data for a typical real-world application: fitting a profile to spectral data.\n The data: stacked velocity-amplitude spectra from a VLA observation The functions:  A modified Gaussian to include Hermite polynomials (approximations to skew and kurtosis) A double gaussian (gaus1 + gaus2 = gausTot)    The data have been downloaded from https://science.nrao.edu/science/surveys/littlethings/data/wlm.html\nimport pyfits cube=pyfits.getdata('WLM_NA_ICL001.FITS')[0,:,:,:] cubehdr=pyfits.getheader('WLM_NA_ICL001.FITS') cdelt3=cubehdr['CDELT3']/1000.; crval3=cubehdr['CRVAL3']/1000.; crpix3=cubehdr['CRPIX3']; minvel=crval3+(-crpix3+1)*cdelt3; maxvel=crval3+(cube.shape[0]-crpix3)*cdelt3 chanwidth=abs(cdelt3) stackspec=np.sum(np.sum(cube,axis=2),axis=1) vels=np.arange(minvel,maxvel+int(cdelt3),cdelt3)  The velocity array in the cube goes from positive to negative, so let‚Äôs reverse it to make the fitting go smoother.\nvels=vels[::-1] stackspec=stackspec[::-1]  We are going to use the default Marquardt-Levenberg algorithm. Note that fitting results will depend quite a bit on what you give as initial guesses ‚Äì ML finds LOCAL extrema quite well, but it doesn‚Äôt necessarily find the global extrema. In short, do your best to provide a good first guess to the fit parameters.\nI said that we want to fit this dataset with a more complex model. Let me explain it a bit before to proced.\n Standard Gaussian:   $ f(x) = A e^{\\frac{-g^2}{2}} $\n  $ g = \\frac{x-x_c}{\\sigma} $\n  Multiple Gaussians:   $ F(x) = \\sum_i f_i(x) = A_1e^{\\frac{-g_1^2}{2}} + A_2e^{\\frac{-g_2^2}{2}} + \\dots $\n  Gauss-Hermite Polynomial:   $f(x) = Ae^{\\frac{-g^2}{2}} [ 1+h_3(-\\sqrt{3}g+\\frac{2}{\\sqrt{3}}g^3 ) + h_4 (\\frac{\\sqrt{6}}{4}-\\sqrt{6}g^2+\\frac{\\sqrt{6}}{3}g^4)] $\n  H_3 ‚Üí (Fisher) Skew: asymmetric component:   $\\xi_1 \\sim 4\\sqrt{3}h_3$\n  H_4 ‚Üí (Fisher) Kurtosis: how \u0026lsquo;fat\u0026rsquo; the tails are:   $xi_2 \\sim 3+8\\sqrt{6}h_4$\n  $\\xi_f = \\xi_2-3$\n  $\\xi_f \\sim 8\\sqrt{6}h_4$$\n Set up the lmfit.Parameters() and define the Gauss-Hermite function:\nfrom lmfit import minimize, Parameters p_gh=Parameters() p_gh.add('amp',value=np.max(stackspec),vary=True); p_gh.add('center',value=vels[50],min=np.min(vels),max=np.max(vels)); p_gh.add('sig',value=3*chanwidth,min=chanwidth,max=abs(maxvel-minvel)); p_gh.add('skew',value=0,vary=True,min=None,max=None); p_gh.add('kurt',value=0,vary=True,min=None,max=None); def gaussfunc_gh(paramsin,x): amp=paramsin['amp'].value center=paramsin['center'].value sig=paramsin['sig'].value c1=-np.sqrt(3); c2=-np.sqrt(6) c3=2/np.sqrt(3); c4=np.sqrt(6)/3; c5=np.sqrt(6)/4 skew=paramsin['skew'].value kurt=paramsin['kurt'].value g=(x-center)/sig gaustot_gh=amp*np.exp(-.5*g**2)*(1+skew*(c1*g+c3*g**3)+ kurt*(c5+c2*g**2+c4*(g**4))) return gaustot_gh  Now do the same for the double gaussian\n Bounds amp : 10% of max to max\ncenter : velocity range\ndisp : channel width to velocity range\n # Double Gaussian (labeled below as ..._2g) p_2g=Parameters() p_2g.add('amp1',value=np.max(stackspec)/2.,min=.1*np.max(stackspec),max=np.max(stackspec)); p_2g.add('center1',value=vels[50+10],min=np.min(vels),max=np.max(vels)); p_2g.add('sig1',value=2*chanwidth,min=chanwidth,max=abs(maxvel-minvel)); p_2g.add('amp2',value=np.max(stackspec)/2.,min=.1*np.max(stackspec),max=np.max(stackspec)); p_2g.add('center2',value=vels[50-10],min=np.min(vels),max=np.max(vels)); p_2g.add('sig2',value=3*chanwidth,min=chanwidth,max=abs(maxvel-minvel)); def gaussfunc_2g(paramsin,x): amp1=paramsin['amp1'].value; amp2=paramsin['amp2'].value; center1=paramsin['center1'].value; center2=paramsin['center2'].value; sig1=paramsin['sig1'].value; sig2=paramsin['sig2'].value; g1=(x-center1)/sig1 g2=(x-center2)/sig2 gaus1=amp1*np.exp(-.5*g1**2) gaus2=amp2*np.exp(-.5*g2**2) gaustot_2g=(gaus1+gaus2) return gaustot_2g  And now the functions that compute the difference between the fit iteration and data. In addition, define a function for a simple single gaussian.\ngausserr_gh = lambda p,x,y: gaussfunc_gh(p,x)-y gausserr_2g = lambda p,x,y: gaussfunc_2g(p,x)-y gausssingle = lambda a,c,sig,x: a*np.exp(-.5*((x-c)/sig)**2)  We will minimize with lmfit, in order to keep limits on parameters:\nfitout_gh=minimize(gausserr_gh,p_gh,args=(vels,stackspec)) fitout_2g=minimize(gausserr_2g,p_2g,args=(vels,stackspec)) fitted_p_gh = fitout_gh.params fitted_p_2g = fitout_2g.params pars_gh=[fitout_gh.params['amp'].value, fitout_gh.params['center'].value, fitout_gh.params['sig'].value, fitout_gh.params['skew'].value, fitout_gh.params['kurt'].value] pars_2g=[fitted_p_2g['amp1'].value, fitted_p_2g['center1'].value, fitted_p_2g['sig1'].value, fitted_p_2g['amp2'].value, fitted_p_2g['center2'].value, fitted_p_2g['sig2'].value]  Finally, if you want to create arrays and residuals of the final fit values:\nfit_gh=gaussfunc_gh(fitted_p_gh,vels) fit_2g=gaussfunc_2g(fitted_p_2g,vels) resid_gh=fit_gh-stackspec resid_2g=fit_2g-stackspec print('Fitted Parameters (Gaus+Hermite):\\nAmp = %.2f , Center = %.2f , Disp = %.2f\\nSkew = %.2f , Kurt = %.2f' \\ %(pars_gh[0],pars_gh[1],pars_gh[2],pars_gh[3],pars_gh[4])) print('Fitted Parameters (Double Gaussian):\\nAmp1 = %.2f , Center1 = %.2f , Sig1 = %.2f\\nAmp2 = %.2f , Center2 = %.2f , Sig2 = %.2f' \\ %(pars_2g[0],pars_2g[1],pars_2g[2],pars_2g[3],pars_2g[4],pars_2g[5]))  Fitted Parameters (Gaus+Hermite): Amp = 178.66 , Center = -119.13 , Disp = 20.68 Skew = -0.12 , Kurt = -0.03 Fitted Parameters (Double Gaussian): Amp1 = 189.58 , Center1 = -112.89 , Sig1 = 14.55 Amp2 = 91.58 , Center2 = -146.19 , Sig2 = 10.26  fig3=plt.figure(3,figsize=(15,10)) f1=fig3.add_axes((.1,.3,.8,.6)) plt.plot(vels,stackspec,'k.') pgh,=plt.plot(vels,fit_gh,'b') p2g,=plt.plot(vels,fit_2g,'r') p2ga,=plt.plot(vels,gausssingle(pars_2g[0],pars_2g[1],pars_2g[2],vels),'-.',color='orange') p2gb,=plt.plot(vels,gausssingle(pars_2g[3],pars_2g[4],pars_2g[5],vels),'-.',color='green') f1.set_xticklabels([]) #We will plot the residuals below, so no x-ticks on this plot plt.title('Multiple Gaussian Fit Example') plt.ylabel('Amplitude (Some Units)') f1.legend([pgh,p2g,p2ga,p2gb],['Gaus-Hermite','2-Gaus','Comp. 1','Comp2'],prop={'size':10},loc='center left') from matplotlib.ticker import MaxNLocator plt.gca().yaxis.set_major_locator(MaxNLocator(prune='lower')) #Removes lowest ytick label f1.annotate('Gauss-Hermite:\\nAmp = %.2f\\nCenter = %.2f\\n$\\sigma$ = %.2f\\nH3 = %.2f\\nH4 = %.2f' \\ %(pars_gh[0],pars_gh[1],pars_gh[2],pars_gh[3],pars_gh[4]),xy=(.05,.95), \\ xycoords='axes fraction',ha=\u0026quot;left\u0026quot;, va=\u0026quot;top\u0026quot;, \\ bbox=dict(boxstyle=\u0026quot;round\u0026quot;, fc='1'),fontsize=10) f1.annotate('Double Gaussian:\\nAmp$_1$ = %.2f\\nAmp$_2$ = %.2f\\nCenter$_1$ = %.2f\\nCenter$_2$ = %.2f\\n$\\sigma_1$ = %.2f\\n$\\sigma_2$ = %.2f' \\ %(pars_2g[0],pars_2g[3],pars_2g[1],pars_2g[4],pars_2g[2],pars_2g[5]),xy=(.95,.95), \\ xycoords='axes fraction',ha=\u0026quot;right\u0026quot;, va=\u0026quot;top\u0026quot;, \\ bbox=dict(boxstyle=\u0026quot;round\u0026quot;, fc='1'),fontsize=10) f2=fig3.add_axes((.1,.1,.8,.2)) resgh,res2g,=plt.plot(vels,resid_gh,'k--',vels,resid_2g,'k') plt.ylabel('Residuals') plt.xlabel('Velocity (km s$^{-1}$)') f2.legend([resgh,res2g],['Gaus-Hermite','2-Gaus'],numpoints=4,prop={'size':9},loc='upper left')  ","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"ada3957d12f5f470f86042f65b696691","permalink":"/post/fitting-functions-to-data/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/fitting-functions-to-data/","section":"post","summary":"Basic ideas about curve fitting, in Python.\n","tags":["least squares","lmfit","optimization","python","scipy"],"title":"Fitting theoretical model to data in python","type":"post"},{"authors":null,"categories":null,"content":"Dataset Dataset is the main object you will work with directly. Dataset is derived from python‚Äôs dict, so it inherits (and overrides some of) the methods of dict. In other words it is a collection of key:value pairs, where the key value is the DICOM (group,element) tag (as a Tag object, described below), and the value is a DataElement instance (also described below).\nA dataset could be created directly, but you will usually get one by reading an existing DICOM file (it could be a .dcm or a .img file):import dicom ds = dicom.read_file(\u0026quot;1111.img\u0026quot;)  You can display the entire dataset by simply printing its string (str or repr) value:\nds  (0008, 0000) Group Length UL: 482 (0008, 0005) Specific Character Set CS: 'ISO_IR 100' (0008, 0008) Image Type CS: ['ORIGINAL', 'PRIMARY'] (0008, 0012) Instance Creation Date DA: '20150710' (0008, 0013) Instance Creation Time TM: '152456' (0008, 0014) Instance Creator UID UI: 1.2.840.113619.1.131 (0008, 0016) SOP Class UID UI: Positron Emission Tomography Image Storage (0008, 0018) SOP Instance UID UI: 1.2.840.113619.2.131.1611270158.1436534696 (0008, 0020) Study Date DA: '20150702' (0008, 0021) Series Date DA: '20150702' (0008, 0022) Acquisition Date DA: '20150702' (0008, 0023) Content Date DA: '20150710' (0008, 0030) Study Time TM: '090706' (0008, 0031) Series Time TM: '091216' (0008, 0032) Acquisition Time TM: '094216' (0008, 0033) Content Time TM: '152456' (0008, 0050) Accession Number SH: '120.116962' (0008, 0060) Modality CS: 'PT' (0008, 0070) Manufacturer LO: 'GE MEDICAL SYSTEMS' (0008, 0080) Institution Name LO: 'FTGM' (0008, 0090) Referring Physician Name PN: '' (0008, 1010) Station Name SH: 'pet94ct' (0008, 1030) Study Description LO: '' (0008, 103e) Series Description LO: 'e+1 NEUROTOTEM' ...  Access header data elements You can access specific data elements by name (DICOM ‚Äòkeyword‚Äô) or by DICOM tag number:\nds.ManufacturerModelName  'Discovery RX'  ds[0x0008,0x1090].value  'Discovery RX'  In the latter case (using the tag number directly) a DataElement instance is returned, so the .value must be used to get the value.\nYou can also set values by name (DICOM keyword) or tag number:\nds.PatientID = \u0026quot;12345\u0026quot; ds.SeriesNumber = 5 ds[0x10,0x10].value = 'Test'  The use of names is possible because PyDicom intercepts requests for member variables, and checks if they are in the DICOM dictionary. It translates the keyword to a (group,element) number and returns the corresponding value for that key if it exists.\nIf you don‚Äôt remember or know the exact tag name, Dataset provides a handy dir() method, useful during interactive sessions at the python prompt:\nds.dir(\u0026quot;pat\u0026quot;)  ['AdditionalPatientHistory', 'ImageOrientationPatient', 'ImagePositionPatient', 'PatientAge', 'PatientBirthDate', 'PatientGantryRelationshipCodeSequence', 'PatientID', 'PatientName', 'PatientOrientationCodeSequence', 'PatientOrientationModifierCodeSequence', 'PatientPosition', 'PatientSex', 'PatientSize', 'PatientWeight']  dir will return any DICOM tag names in the dataset that have the specified string anywhere in the name (case insensitive).\n NOTE: Calling dir with no string will list all tag names available in the dataset. You can also see all the names that pydicom knows about by viewing the _dicom_dict.py file. You could \u0026gt;modify that file to add tags that pydicom doesn‚Äôt already know about.\n Under the hood, Dataset stores a DataElement object for each item, but when accessed by name (e.g. ds.PatientName) only the value of that DataElement is returned. If you need the whole DataElement (see the DataElement class discussion), you can use Dataset‚Äôs data_element() method or access the item using the tag number:\ndata_element = ds.data_element(\u0026quot;PatientsName\u0026quot;) # or data_element = ds[0x10,0x10] data_element.VR, data_element.value  ('PN', 'Test')  DataElement The DataElement class is not usually used directly in user code, but is used extensively by Dataset. DataElement is a simple object which stores the following things:\n tag ‚Äì a DICOM tag (as a Tag object) VR ‚Äì DICOM value representation ‚Äì various number and string formats, etc VM ‚Äì value multiplicity. This is 1 for most DICOM tags, but can be multiple, e.g. for coordinates. You do not have to specify this, the DataElement class keeps track of it based on value. value ‚Äì the actual value. A regular value like a number or string (or list of them), or a Sequence.  To check for the existence of a particular tag before using it, use the in keyword:\n\u0026quot;PatientName\u0026quot; in ds  True  To remove a data element from the dataset, use del:\ndel ds.SoftwareVersions # or del ds[0x0018, 0x1020] \u0026quot;SoftwareVersions\u0026quot; in ds  False  TAG Using DICOM keywords is the recommended way to access data elements, but you can also use the tag numbers directly. The Tag class is derived from python‚Äôs int, so in effect, it is just a number with some extra behaviour:\n Tag enforces that the DICOM tag fits in the expected 4-byte (group,element) A Tag instance can be created from an int or from a tuple containing the (group,element) separately Tag has properties group and element (or elem) to return the group and element portions The is_private property checks whether the tag represents a private tag (i.e. if group number is odd).  from dicom.tag import Tag t1=Tag(0x00100010) # all of these are equivalent t2=Tag(0x10,0x10) t3=Tag((0x10, 0x10)) print t1 t1==t2, t1==t3 (True, True)  (0010, 0010) (True, True)  Access image data DICOM Sequences are turned into python lists or strings. Items in the sequence are referenced by number, beginning at index 0 as per python convention. \u0026ldquo;Sequence\u0026rdquo; data type is derived from python‚Äôs list. The only added functionality is to make string representations prettier. Otherwise all the usual methods of list like item selection, append, etc. are available. To work with pixel data, the raw bytes are available through the usual tag:\nimage_data = ds.PixelData print type(image_data)  \u0026lt;type 'str'\u0026gt;  We then calculate the total dimensions of the NumPy array which are equal to (Number of pixel rows in a slice) x (Number of pixel columns in a slice) x (Number of slices) along the x, y, and z cartesian axes. In this example we are dealing with just a single slice DICOM file, so z=1.\nLastly, we use the PixelSpacing and SliceThickness attributes to calculate the spacing between pixels in the three axes. We store the array dimensions in ConstPixelDims and the spacing in ConstPixelSpacing.\nimport numpy as np ConstPixelDims = (int(ds.Rows), int(ds.Columns)) ConstPixelSpacing = (float(ds.PixelSpacing[0]), float(ds.PixelSpacing[1]), float(ds.SliceThickness)) print ConstPixelDims print ConstPixelSpacing  (128, 128) (3.125, 3.125, 3.27)  Here, we simply use numpy.arange, ConstPixelDims, and ConstPixelSpacing to calculate axes for this array:\nx = np.arange(0.0, (ConstPixelDims[0]+1)*ConstPixelSpacing[0], ConstPixelSpacing[0]) y = np.arange(0.0, (ConstPixelDims[1]+1)*ConstPixelSpacing[1], ConstPixelSpacing[1])  Next, comes the last pydicom part:\n# The array is sized based on 'ConstPixelDims' ArrayDicom = np.zeros(ConstPixelDims, dtype=ds.pixel_array.dtype) ArrayDicom[:,:] = ds.pixel_array  %matplotlib inline from matplotlib import pyplot, cm pyplot.figure(dpi=300) pyplot.axes().set_aspect('equal') pyplot.set_cmap(pyplot.gray()) pyplot.pcolormesh(x, y, np.flipud(ArrayDicom[:, :])) pyplot.show()  ","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"c1ee77485f25ba6f279127cf4ec2bb8f","permalink":"/post/read_dicom_files_in_python/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/read_dicom_files_in_python/","section":"post","summary":"Read header information and load data from DICOM files, in Python.\n","tags":["dicom","medical imaging","python"],"title":"How to read DICOM files into Python","type":"post"},{"authors":null,"categories":null,"content":"Dimensionality Reduction: Principal Component Analysis in-depth Here we\u0026rsquo;ll explore Principal Component Analysis, which is an extremely useful linear dimensionality reduction technique.\nWe\u0026rsquo;ll start with our standard set of initial imports:from __future__ import print_function, division %matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy import stats # use seaborn plotting style defaults import seaborn as sns; sns.set()  Introducing Principal Component Analysis Principal Component Analysis is a very powerful unsupervised method for dimensionality reduction in data. It\u0026rsquo;s easiest to visualize by looking at a two-dimensional dataset:\nnp.random.seed(1) X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T plt.plot(X[:, 0], X[:, 1], 'o') plt.axis('equal') print(X.shape)  (200, 2)  We can see that there is a definite trend in the data. What PCA seeks to do is to find the Principal Axes in the data, and explain how important those axes are in describing the data distribution:\nfrom sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X) print(pca.explained_variance_) print(pca.components_)  [ 0.75871884 0.01838551] [[ 0.94446029 0.32862557] [ 0.32862557 -0.94446029]]  To see what these numbers mean, let\u0026rsquo;s view them as vectors plotted on top of the data:\nplt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5) for length, vector in zip(pca.explained_variance_, pca.components_): v = vector * 3 * np.sqrt(length) plt.plot([0, v[0]], [0, v[1]], '-k', lw=3) plt.axis('equal');  Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \u0026ldquo;important\u0026rdquo; than the other direction. The explained variance quantifies this measure of \u0026ldquo;importance\u0026rdquo; in direction.\nAnother way to think of it is that the second principal component could be completely ignored without much loss of information! Let\u0026rsquo;s see what our data look like if we only keep 95% of the variance:\nclf = PCA(0.95) # keep 95% of variance X_trans = clf.fit_transform(X) print(X.shape) print(X_trans.shape)  (200, 2) (200, 1)  By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let\u0026rsquo;s see what the data look like after this compression:\nX_new = clf.inverse_transform(X_trans) x_plot = plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.4, label='X') xnew_plot = plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8, label='X_new') plt.axis('equal') plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7ff9f16f7f50\u0026gt;  The light points are the original data, while the dark points are the projected version. We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \u0026ldquo;most important\u0026rdquo; features of the data are maintained, and we\u0026rsquo;ve compressed the data by 50%!\nThis is the sense in which \u0026ldquo;dimensionality reduction\u0026rdquo; works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data.\nWhat do the Components Mean? PCA is a very useful dimensionality reduction algorithm, because it has a very intuitive interpretation via eigenvectors. The input data is represented as a vector: If we reduce the dimensionality in the pixel space to (say) 6, we recover only a partial image.\nfrom sklearn.datasets import load_digits digits = load_digits() X = digits.data y = digits.target  What PCA does is to choose optimal basis functions so that only a few are needed to get a reasonable approximation. The low-dimensional representation of our data is the coefficients of this series, and the approximate reconstruction is the result of the sum:\nChoosing the Number of Components But how much information have we thrown away? We can figure this out by looking at the explained variance as a function of the components:\nsns.set() pca = PCA().fit(X) plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('number of components') plt.ylabel('cumulative explained variance');  Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we\u0026rsquo;d need about 20 components to retain 90% of the variance. Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.\n","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"94146546251eb0d4a8a1bbdaf186f514","permalink":"/post/pca-tutorial-using-scikit-learn-python-module/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/pca-tutorial-using-scikit-learn-python-module/","section":"post","summary":"Here we'll explore Principal Component Analysis, which is an extremely useful linear dimensionality reduction technique. Using Scikit-learn and Python.\n","tags":["PCA","python","scikit-learn"],"title":"PCA tutorial using scikit-learn python module","type":"post"},{"authors":null,"categories":null,"content":"The problem Today we are going to test a very simple example of nonlinear least squares curve fitting using the scipy.optimize module.\n%matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy.optimize import curve_fit  Create data Let\u0026rsquo;s assume we have the following points [xdata, ydata] and that we want to fit these data with the following model function using nonlinear least squares:\n$F(p_1,p_2,x) = p_1\\cos(p_2x) + p_2\\sin(p_1x)$\nFor now, we are primarily interested in the following results:\n The fit parameters Sum of squared residuals  xdata = np.array([-2,-1.64,-1.33,-0.7,0,0.45,1.2,1.64,2.32,2.9]) ydata = np.array([0.699369,0.700462,0.695354,1.03905,1.97389,2.41143,1.91091,0.919576,-0.730975,-1.42001]) # Show data points plt.plot(xdata,ydata,'*') plt.xlabel('xdata') plt.ylabel('ydata');  Define fit function def func(x, p1,p2): return p1*np.cos(p2*x) + p2*np.sin(p1*x)  Calculate and show fit parameters. Use a starting guess of $p_1=1$ and $p_2=0.2$\nThe outputs of the curve_fit function are the following:\n  popt : array of optimal values for the parameters so that the sum of the squared error of $f(xdata, *popt) - ydata$ is minimized\n  pcov : 2d array of the estimated covariance of popt. The diagonals provide the variance of the parameter estimate. To compute one standard deviation errors on the parameters use $perr = np.sqrt(np.diag(pcov))$. If the Jacobian matrix at the solution doesn\u0026rsquo;t have a full rank, then \u0026lsquo;lm\u0026rsquo; method returns a matrix filled with np.inf, on the other hand \u0026lsquo;trf\u0026rsquo; and \u0026lsquo;dogbox\u0026rsquo; methods use Moore-Penrose pseudoinverse to compute the covariance matrix.\n  popt, pcov = curve_fit(func, xdata, ydata,p0=(1.0,0.2)) print(\u0026quot;Parameter estimation results:\u0026quot;) print(\u0026quot;p1 = \u0026quot;,popt[0],\u0026quot; | p2 = \u0026quot;,popt[1]) print(\u0026quot;--------------------------\u0026quot;) print(\u0026quot;Covariance matrix of the estimate:\u0026quot;) print(pcov)  Parameter estimation results: p1 = 1.881850994 | p2 = 0.700229857403 -------------------------- Covariance matrix of the estimate: [[ 7.52408290e-04 1.00812823e-04] [ 1.00812823e-04 8.37695698e-05]]  Sum of squares of residuals Since it\u0026rsquo;s not given by the curve_fit function, we have to compute it by hand\np1 = popt[0] p2 = popt[1] residuals = ydata - func(xdata,p1,p2) fres = sum(residuals**2) print(\u0026quot;Residuals sum of squares:\u0026quot;) print(fres)  Residuals sum of squared: 0.0538126964188  Plot fitted curve along with data\ncurvex=np.linspace(-2,3,100) curvey=func(curvex,p1,p2) plt.plot(xdata,ydata,'*') plt.plot(curvex,curvey,'r') plt.xlabel('xdata') plt.ylabel('ydata');  ","date":1462100400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462100400,"objectID":"b77385e71ff256ff7885e7df30e2d7a7","permalink":"/post/python_nonlinear_least_squares/","publishdate":"2016-05-01T11:00:00Z","relpermalink":"/post/python_nonlinear_least_squares/","section":"post","summary":"Testing a very simple example of nonlinear least squares curve fitting using the scipy.optimize module.\n","tags":["curve fitting","optimization","python","scipy"],"title":"Simple nonlinear least squares curve fitting in Python","type":"post"},{"authors":null,"categories":null,"content":"The Academic framework enables you to easily create a beautifully simple personal or academic website using the Hugo static site generator.\nKey features:\n Easily manage your homepage, blog posts, publications, talks, and projects Configurable widgets available for Biography, Publications, Projects, News/Blog, Talks, and Contact Need a different section? Just use the Custom widget! Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Easy to customize  Installation    Install Hugo and create a new website by typing the following commands in your Terminal or Command Prompt app:\n hugo new site my_website cd my_website    Install Academic with git:\n git clone https://github.com/gcushen/hugo-academic.git themes/academic  Or alternatively, download Academic and extract it into a themes/academic folder within your Hugo website.\n  If you are creating a new website, copy the contents of the exampleSite folder to your website root folder, overwriting existing files if necessary. The exampleSite folder contains an example config file and content to help you get started.\n cp -av themes/academic/exampleSite/* .    Start the Hugo server from your website root folder:\n hugo server --watch  Now you can go to localhost:1313 and your new Academic powered website should appear.\n  Customize your website - refer to the Getting Started section below\n  Build your site by running the hugo command. Then host it for free using Github Pages. Or alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as your university\u0026rsquo;s hosting service).\n  Getting Started Assuming you created a new website with the example content following the installation steps above, this section explores just a few more steps in order to customize it.\nCore parameters The core parameters for the website can be edited in the config.toml configuration file:\n Set baseurl to your website URL (we recommend GitHub Pages for free hosting) Set title to your desired website title such as your name The example Disqus commenting variable should be cleared (e.g. disqusShortname = \u0026quot;\u0026quot;) or set to your own Disqus shortname to enable commenting Edit your details under [params]; these will be displayed mainly in the homepage about and contact widgets (if used). To disable a contact field, simply clear the value to \u0026quot;\u0026quot;. Place a square cropped portrait photo named portrait.jpg into the static/img/ folder, overwriting any defaults. Alternatively, you can edit the avatar filepath to point to a different image name or clear the value to disable the avatar feature. To enable LaTeX math for your site, set math = true Social/academic networking links are defined as multiples of [[params.social]]. They can be created or deleted as necessary.  Introduce yourself Edit your biography in the about widget content/home/about.md that you copied across from the themes/academic/exampleSite/ folder. The research interests and qualifications are stored as interests and education variables. The academic qualifications are defined as multiples of [[education.courses]] and can be created or deleted as necessary. It\u0026rsquo;s possible to completely hide the interests and education lists by deleting their respective variables.\nCustomize the homepage Refer to our guide on using widgets to customize your homepage.\nAdd your content Refer to our guide on managing content to create your own publications, blog posts, talks, and projects.\nRemove unused widgets and pages  How to remove unused widgets and content pages.\nCustomization \u0026amp; Upgrading Continue reading below for advanced customization tips and instructions for keeping the framework up-to-date with any improvements that become available.\nAdvanced customization It is possible to carry out many customizations without touching any files in themes/academic, making it easier to upgrade the framework in the future.\nNavigation menu The [[menu.main]] entries towards the bottom of config.toml define the navigation links at the top of the website. They can be added or removed as desired.\nTo create a dropdown sub-menu, add identifier = \u0026quot;something\u0026quot; to the parent item and parent = \u0026quot;something\u0026quot; to the child item.\nWebsite icon Save your main icon and mobile icon as square PNG images named icon.png and apple-touch-icon.png, respectively. Place them in your root static/img/ folder.\nTheme color (CSS) You can link custom CSS assets (relative to your root static/css) from your config.toml using custom_css = [\u0026quot;custom.css\u0026quot;].\nFor example, lets make a green theme. First, define custom_css = [\u0026quot;green.css\u0026quot;] in config.toml. Then we can download the example green theme and save it as static/css/green.css, relative to your website root (i.e. not in the themes directory).\nAnalytics To enable Google Analytics, add your tracking code in config.toml similarly to googleAnalytics = \u0026quot;UA-12345678-9\u0026quot;.\nThird party and local scripts (JS) To add a third party script, create a file named head_custom.html in a layouts/partials/ folder at the root of your website (not in the themes folder). Any HTML code added to this file will be included within your website\u0026rsquo;s \u0026lt;head\u0026gt;. Therefore, it\u0026rsquo;s suitable for adding custom metadata or third party scripts specified with the async attribute.\nWhereas for your own local scripts, you can link your local JS assets (relative to your root static/js) from your config.toml using custom_js = [\u0026quot;custom.js\u0026quot;].\nLanguage and translation The interface text (e.g. buttons) is stored in language files which are collected from Academic\u0026rsquo;s themes/academic/i18n/ folder, as well as an i18n/ folder at the root of your project.\nTo edit the interface text, copy themes/academic/i18n/en.yaml to i18n/en.yaml (relative to the root of your website). Open the new file and make any desired changes to the text appearing after translation:. Note that the language files are formatted in YAML syntax.\nTo translate the interface text to another language, follow the above instructions, but name the new file in the form i18n/X.yaml where X is the appropriate ISO/RFC5646 language identifier for the translation. Then follow the brief instructions in the Language section at the bottom of your config.toml. To change the default language used by Academic, set defaultContentLanguage to the desired language identifier in your configuration file.\nTo translate the navigation bar, you can edit the default [[menu.main]] instances in config.toml. However, for a multilingual site, you will need to duplicate all of the [[menu.main]] instances and rename the new instances from [[menu.main]] to [[languages.X.menu.main]], where X is the language identifier (e.g. [[languages.zh.menu.main]] for Simplified Chinese). Thus, the navigation bar can be displayed in multiple languages.\nTo translate a content file in your content/ folder into another language, copy the file to filename.X.md where filename is your existing filename and X is the appropriate ISO/RFC5646 language identifier for the translation. Then translate the content in the new file to the specified language.\nFor further details on Hugo\u0026rsquo;s internationalization and multilingual features, refer to the associated Hugo documentation.\nPermalinks Permalinks, or permanent links, are URLs to individual pages and posts on your website. They are permanent web addresses which can be used to link to your content. Using Hugo\u0026rsquo;s permalinks option these can be easily customized. For example, the blog post URL can be changed to the form yourURL/2016/05/01/my-post-slug by adding the following near the top of your config.toml (before [params] settings):\n[permalinks] post = \u0026quot;/:year/:month/:day/:slug\u0026quot;  Where :slug defaults to the filename of the post, excluding the file extension. However, slug may be overridden on a per post basis if desired, simply by setting slug = \u0026quot;my-short-post-title\u0026quot; in your post preamble.\nUpgrading Feel free to star the project on Github and monitor the commits for updates.\nBefore upgrading the framework, it is recommended to make a backup of your entire website directory, or at least your themes/academic directory. You can also read about the most recent milestones (but this doesn\u0026rsquo;t necessarily reflect the latest master release).\nBefore upgrading for the first time, the remote origin repository should be renamed to upstream:\n$ cd themes/academic $ git remote rename origin upstream  To list available updates:\n$ cd themes/academic $ git fetch upstream $ git log --pretty=oneline --abbrev-commit --decorate HEAD..upstream/master  Then, upgrade by running:\n$ git pull upstream  If you have modified files in themes/academic, git will attempt to auto-merge changes. If conflicts are reported, you will need to manually edit the files with conflicts and add them back (git add \u0026lt;filename\u0026gt;).\nIf there are any issues after upgrading, you may wish to compare your site with the latest example site to check if any settings changed.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor general questions about Hugo, there is a Hugo discussion forum.\nLicense Copyright 2016 George Cushen.\nReleased under the MIT license.\n","date":1461153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461153600,"objectID":"ba6423d815d4f5949b7a69912feb741d","permalink":"/post/getting-started/","publishdate":"2016-04-20T12:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple personal or academic website in under 10 minutes.\n","tags":["academic","hugo","tutorial"],"title":"Getting started with the Academic framework for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Homepage widgets display as sections on the homepage. They can be enabled/disabled and configured as desired. Academic has the following widgets available to use:\n About/biography Selected publications Recent publications Recent news/blog posts Projects Selected talks Recent talks Contact Custom widget (demonstrated with the teaching example)  The example site that you copied to create your site uses all the different types of widget (except talks), so you can generally just delete the widgets you don\u0026rsquo;t need and customize the parameters of the widgets you wish to keep.\nThe parameters for each widget vary. They can be found in the preamble/frontmatter (between the pair of +++) for each widget installed in the content/home/ folder.\n By default, publications will be displayed in a simple list. If you prefer a more detailed list with abstract and image, you can enable the detailed publication list on the homepage by setting detailed_list = true in content/home/publications.md.   Add a widget to the homepage To add a widget manually, copy the relevant widget from themes/academic/exampleSite/content/home/ to your content/home/ folder.\nWidget identifiers are set to their respective filenames, so a content/home/about.md widget can be linked from the navigation bar by setting the relevant URL as \u0026quot;#about\u0026quot; in config.toml.\nThis means that if you want to use multiple instances of a widget, each widget will be assigned a unique ID based on the filename that you set. You can then use that ID for linking, like in the above example.\nUsing the custom widget You can use the custom widget to create your own home page sections.\nSimply duplicate (copy/paste) and rename the example teaching file at content/home/teaching.md. Then edit the section title, weight (refer to Ordering sections below), and content as desired.\nYou may also wish to add a navigation link to the top of the page that points to the new section. This can be achieved by adding something similar to the following lines to your config.toml, where the URL will consist of the first title word in lowercase:\n[[menu.main]] name = \u0026quot;Research\u0026quot; url = \u0026quot;#research\u0026quot; weight = 10  Remove a widget from the homepage If you do not require a particular widget, you can simply delete any associated files from the content/home/ folder.\nTo remove a navigation link from the top of the page, remove the associated [[menu.main]] entry in config.toml.\nOrdering widgets The order that the homepage widgets are displayed in is defined by the weight parameter in each of the files in the content/home/ directory. The widgets are displayed in ascending order of their weight, so you can simply edit the weight parameters as desired.\n","date":1461150000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461150000,"objectID":"81e1c59711f282aead8f477db85a3623","permalink":"/post/widgets/","publishdate":"2016-04-20T11:00:00Z","relpermalink":"/post/widgets/","section":"post","summary":"Enable/disable and configure widgets to customize your homepage.\n","tags":["academic","hugo","tutorial"],"title":"Customizing the homepage with widgets","type":"post"},{"authors":null,"categories":null,"content":"This is a brief guide to managing content with the Academic framework. Content can include publications, projects, talks, and news/blog articles. After you have read this guide about creating and managing content, you may also be interested to learn about writing content with Markdown, LaTeX, and Shortcodes.\nTo enable LaTeX math rendering for a page, you should include math = true in the page\u0026rsquo;s +++ preamble, as demonstrated in the included example site. Otherwise, to enable math on the homepage or for all pages, you must globally set math = true in config.toml.\nTo disable source code highlighting by default for all pages, set highlight = false in config.toml. You can then enable source code highlighting only on pages that need it by setting highlight = true in that page\u0026rsquo;s preamble. See the code-highlighting docs for more details.\nTo display a featured image in content page headers, the parameters below can be inserted towards the end of a page\u0026rsquo;s +++ preamble. It is assumed that the image is located in your static/img/ folder, so the full path in the example below will be static/img/headers/getting-started.png. The caption parameter can be used to write an image caption or credit.\n[header] image = \u0026quot;headers/getting-started.png\u0026quot; caption = \u0026quot;Image credit: [**Academic**](https://github.com/gcushen/hugo-academic/)\u0026quot;   If you wish to prevent a featured image automatically being used for a post\u0026rsquo;s thumbnail on the homepage, the preview = false parameter can be added to [header].   Create a publication To create a new publication:\nhugo new publication/my-paper-name.md  Then edit the default variables at the top of content/publication/my-paper-name.md to include the details of your publication. The url_ variables are used to generate links associated with your publication, such as for viewing PDFs of papers. Here is an example:\n+++ abstract = \u0026quot;An abstract...\u0026quot; authors = [\u0026quot;First author's name\u0026quot;, \u0026quot;Second author's name\u0026quot;] date = \u0026quot;2013-07-01\u0026quot; image = \u0026quot;\u0026quot; image_preview = \u0026quot;\u0026quot; math = false publication = \u0026quot;The publishing part of the citation goes here. You may use *Markdown* for italics etc.\u0026quot; title = \u0026quot;A publication title, such as title of a paper\u0026quot; url_code = \u0026quot;\u0026quot; url_dataset = \u0026quot;\u0026quot; url_pdf = \u0026quot;pdf/my-paper-name.pdf\u0026quot; url_project = \u0026quot;\u0026quot; url_slides = \u0026quot;\u0026quot; url_video = \u0026quot;\u0026quot; +++ Further details on your publication can be written here using *Markdown* for formatting. This text will be displayed on the Publication Detail page.  The url_ links can either point to local or web content. Associated local publication content, such as PDFs, may be copied to a static/pdf/ folder and referenced like url_pdf = \u0026quot;pdf/my-paper-name.pdf\u0026quot;.\nYou can also associate custom link buttons with the publication by adding the following block(s) within the variable preamble above, which is denoted by +++:\n[[url_custom]] name = \u0026quot;Custom Link\u0026quot; url = \u0026quot;http://www.example.org\u0026quot;  If you enabled detailed_list for publications in config.toml, then there are a few more optional variables that you can include in the publication page preamble. You may use abstract_short = \u0026quot;friendly summary of abstract\u0026quot; and publication_short = \u0026quot;abbreviated publication details\u0026quot; to display a friendly summary of the abstract and abbreviate the publication details, respectively. Furthermore, there is the option to display a different image on the homepage to the publication detail page by setting image_preview = \u0026quot;my-image.jpg\u0026quot;. This can be useful if you wish to scale down the image for the homepage or simply if you just wish to show a different image for the preview.\n Any double quotes (\u0026quot;) or backslashes (e.g. LaTeX \\times) occurring within the value of any frontmatter parameter (such as the abstract) should be escaped with a backslash (\\). For example, the symbol \u0026quot; and LaTeX text \\times become \\\u0026quot; and \\\\times, respectively. Refer to the TOML documentation for more info.   Post an article To create a blog/news article:\nhugo new post/my-article-name.md  Then edit the newly created file content/post/my-article-name.md with your full title and content.\nHugo will automatically generate summaries of posts that appear on the homepage. If you are dissatisfied with an automated summary, you can either limit the summary length by appropriately placing \u0026lt;!--more--\u0026gt; in the article body, or completely override the automated summary by adding a summary parameter to the +++ preamble such that:\nsummary = \u0026quot;Summary of my post.\u0026quot;  To disable commenting for a specific post, you can add disable_comments = true to the post +++ preamble. Or to disable commenting for all posts, you can either set disqusShortname = \u0026quot;\u0026quot; or disable_comments = true in config.toml.\nCreate a project To create a project:\nhugo new project/my-project-name.md  Then edit the newly created file content/project/my-project-name.md. Either you can link the project to an external project website by setting the external_link = \u0026quot;http://external-project.com\u0026quot; variable at the top of the file, or you can add content (below the final +++) in order to render a project page on your website.\nCreate a talk To create a talk:\nhugo new talk/my-talk-name.md  Then edit the newly created file content/talk/my-talk-name.md with your full talk title and details. Note that many of the talk parameters are similar to the publication parameters.\nManage node index pages The node index pages (e.g. /post/) are the special pages which list all of your content. They can exist for blog posts, publications, and talks. The homepage widgets will automatically link to the node index pages when you have more items of content than can be displayed in the widget. Therefore, if you don\u0026rsquo;t have much content, you may not see the automatic links yet - but you can also manually link to them using a normal Markdown formatted link in your content.\nYou can edit the title and add your own content, such as an introduction, by creating and editing the following content files for the node indexes:\nhugo new post/_index.md hugo new publication/_index.md hugo new talk/_index.md  Then remove all parameters except for title, math, highlight, and date. Edit the title parameter as desired and add any content after the +++ preamble/frontmatter ends. For example, you should have something similar to:\n+++ title = \u0026quot;List of my posts\u0026quot; date = \u0026quot;2017-01-01T00:00:00Z\u0026quot; math = false highlight = false +++ Below is an automatically generated list of all my blog posts!  Removing content Generally, to remove content, simply delete the relevant file from your content/post, content/publication, content/project, or content/talk folder.\nView your updated site After you have made changes to your site, you can view it by running the hugo server --watch command and then opening localhost:1313 in your web browser.\nDeploy your site Finally, you can build the static website to a public/ folder ready for deployment using the hugo command.\nYou may then deploy your site by copying the public/ directory (by FTP, SFTP, WebDAV, Rsync, git push, etc.) to your production web server.\nNote that running hugo does not remove any previously generated files before building. Therefore, it\u0026rsquo;s best practice to delete your public/ directory prior to running hugo to ensure no old or interim files are deployed to your server.\n","date":1461150000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461150000,"objectID":"70a08859314cba3ac79d2c0c856d1fcd","permalink":"/post/managing-content/","publishdate":"2016-04-20T11:00:00Z","relpermalink":"/post/managing-content/","section":"post","summary":"This is a brief guide to managing content with the Academic framework. Content can include publications, projects, talks, and news/blog articles. After you have read this guide about creating and managing content, you may also be interested to learn about writing content with Markdown, LaTeX, and Shortcodes.\n","tags":["academic","hugo","tutorial"],"title":"Managing content","type":"post"},{"authors":null,"categories":null,"content":"Content can be written using Markdown, LaTeX math, and Hugo Shortcodes. Additionally, HTML may be used for advanced formatting.\nThis article gives an overview of the most common formatting options.\nSub-headings ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6  Emphasis Italics with *asterisks* or _underscores_. Bold with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough with ~~two tildes~~.  Ordered lists 1. First item 2. Another item  Unordered lists * First item * Another item  Images Images may be added to a page by placing them in your static/img/ folder and referencing them using one of the following two notations:\nA general image:\n![alternative text for search engines](/img/screenshot.png)  A numbered figure with caption:\n{{\u0026lt; figure src=\u0026quot;/img/screenshot.png\u0026quot; title=\u0026quot;Figure Caption\u0026quot; \u0026gt;}}  Links [I'm a link](https://www.google.com) [A post]({{\u0026lt; ref \u0026quot;post/hi.md\u0026quot; \u0026gt;}}) [A publication]({{\u0026lt; ref \u0026quot;publication/hi.md\u0026quot; \u0026gt;}}) [A project]({{\u0026lt; ref \u0026quot;project/hi.md\u0026quot; \u0026gt;}}) [Another section]({{\u0026lt; relref \u0026quot;hi.md#who\u0026quot; \u0026gt;}})  Emojis See the Emoji cheat sheet for available emoticons. The following serves as an example, but you should remove the spaces between each emoji name and pair of semicolons:\nI : heart : Academic : smile :  I ‚ù§ Academic üòÑ\nBlockquote \u0026gt; This is a blockquote.   This is a blockquote.\n Footnotes I have more [^1] to say. [^1]: Footnote example.  I have more 1 to say.\nCode highlighting Pass the language of the code, such as python, as a parameter after three backticks:\n```python # Example of code highlighting input_string_var = input(\u0026quot;Enter some data: \u0026quot;) print(\u0026quot;You entered: {}\u0026quot;.format(input_string_var)) ```  Result:\n# Example of code highlighting input_string_var = input(\u0026quot;Enter some data: \u0026quot;) print(\u0026quot;You entered: {}\u0026quot;.format(input_string_var))  Highlighting options The Academic theme uses highlight.js for source code highlighting, and highlighting is enabled by default for all pages. However, several configuration options are supported that allow finer-grained control over highlight.js.\nThe following table lists the supported options for configuring highlight.js, along with their expected type and a short description. A \u0026ldquo;yes\u0026rdquo; in the config.toml column means the value can be set globally in config.toml, and a \u0026ldquo;yes\u0026rdquo; in the preamble column means that the value can be set locally in a particular page\u0026rsquo;s preamble.\n   option type description config.toml preamble     highlight boolean enable/disable highlighting yes yes   highlight_languages slice choose additional languages yes yes   highlight_style string choose a highlighting style yes no   highlight_version string choose the highlight.js version yes no    Option highlight The highlight option allows enabling or disabling the inclusion of highlight.js, either globally or for a particular page. If the option is unset, it has the same effect as if you had specified highlight = true. That is, the highlight.js javascript and css files will be included in every page. If you\u0026rsquo;d like to only include highlight.js files on pages that actually require source code highlighting, you can set highlight = false in config.toml, and then override it by setting highlight = true in the preamble of any pages that require source code highlighting. Conversely, you could enable highlighting globally, and disable it locally for pages that do not require it. Here is a table that shows whether highlighting will be enabled for a page, based on the values of highlight set in config.toml and/or the page\u0026rsquo;s preamble.\n   config.toml page preamble highlighting enabled for page?     unset or true unset or true yes   unset or true false no   false unset or false no   false true yes    Option highlight_languages The highlight_languages option allows you to specify additional languages that are supported by highlight.js, but are not considered \u0026ldquo;common\u0026rdquo; and therefore are not supported by default. For example, if you want source code highlighting for Go and clojure in all pages, set highlight_languages = [\u0026quot;go\u0026quot;, \u0026quot;clojure\u0026quot;] in config.toml. If, on the other hand, you want to enable a language only for a specific page, you can set highlight_languages in that page\u0026rsquo;s preamble.\nThe highlight_languages options specified in config.toml and in a page\u0026rsquo;s preamble are additive. That is, if config.toml contains, highlight_languages = [\u0026quot;go\u0026quot;] and the page\u0026rsquo;s preamble contains highlight_languages = [\u0026quot;ocaml\u0026quot;], then javascript files for both go and ocaml will be included for that page.\nIf the highlight_languages option is set, then the corresponding javascript files will be served from the cdnjs server. To see a list of available languages, visit the cdnjs page and search for links with the word \u0026ldquo;languages\u0026rdquo;.\nThe highlight_languages option provides an easy and convenient way to include support for additional languages to be severed from a CDN. If serving unmodified files from cdnjs doesn\u0026rsquo;t meet your needs, you can include javascript files for additional language support via one of the methods described in the getting started guide.\nOption highlight_style The highlight_style option allows you to select an alternate css style for highlighted code. For example, if you wanted to use the solarized-dark style, you could set highlight_style = \u0026quot;solarized-dark\u0026quot; in config.toml.\nIf the highlight_style option is unset, the default is to use the file /css/highlight.min.css, either the one provided by the Academic theme, or else the one in your local static directory. The /css/highlight.min.css file provided by Academic is equivalent to the github style from highlight.js.\nIf the highlight_style option is set, then /css/highlight.min.css is ignored, and the corresponding css file will be served from the cdnjs server. To see a list of available styles, visit the cdnjs page and search for links with the word \u0026ldquo;styles\u0026rdquo;.\nSee the highlight.js demo page for examples of available styles.\n Not all styles listed on the highlight.js demo page are available from the cdnjs server. If you want to use a style that is not served by cdnjs, just leave highlight_style unset, and place the corresponding css file in /static/css/highlight.min.css.    If you don\u0026rsquo;t want to change the default style that ships with Academic but you do want the style file served from the cdnjs server, set highlight_style = \u0026quot;github\u0026quot; in config.toml.   The highlight_style option is only recognized when set in config.toml. Setting highlight_style in your page\u0026rsquo;s preamble has no effect.\nOption highlight_version The highlight_version option, as the name implies, allows you to select the version of highlight.js you want to use. The default value is \u0026ldquo;9.9.0\u0026rdquo;. The highlight_version option is only recognized when set in config.toml. Setting highlight_version in your page\u0026rsquo;s preamble has no effect.\nTwitter tweet To include a single tweet, pass the tweet‚Äôs ID from the tweet\u0026rsquo;s URL as parameter to the shortcode:\n{{\u0026lt; tweet 666616452582129664 \u0026gt;}}  Youtube {{\u0026lt; youtube w7Ft2ymGmfc \u0026gt;}}  Vimeo {{\u0026lt; vimeo 146022717 \u0026gt;}}  GitHub gist {{\u0026lt; gist USERNAME GIST-ID \u0026gt;}}  Speaker Deck {{\u0026lt; speakerdeck 4e8126e72d853c0060001f97 \u0026gt;}}  $\\rm \\LaTeX$ math $$\\left [ ‚Äì \\frac{\\hbar^2}{2 m} \\frac{\\partial^2}{\\partial x^2} + V \\right ] \\Psi = i \\hbar \\frac{\\partial}{\\partial t} \\Psi$$  $$\\left [ ‚Äì \\frac{\\hbar^2}{2 m} \\frac{\\partial^2}{\\partial x^2} + V \\right ] \\Psi = i \\hbar \\frac{\\partial}{\\partial t} \\Psi$$\nAlternatively, inline math can be written by wrapping the formula with only a single $:\nThis is inline: $\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon$  This is inline: $\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon$\nTable Code:\n| Command | Description | | ------------------| ------------------------------ | | `hugo` | Build your website. | | `hugo serve -w` | View your website. |  Result:\n   Command Description     hugo Build your website.   hugo serve -w View your website.    Alerts Alerts are a useful feature that add side content such as tips, notes, or warnings to your articles. They are especially handy when writing educational tutorial-style articles. Use the corresponding shortcodes to enable alerts inside your content:\n{{% alert note %}} Here's a tip or note... {{% /alert %}}  This will display the following note block:\n Here\u0026rsquo;s a tip or note\u0026hellip;   {{% alert warning %}} Here's some important information... {{% /alert %}}  This will display the following warning block:\n Here\u0026rsquo;s some important information\u0026hellip;     Footnote example. \u0026#x21a9;\u0026#xfe0e;\n  ","date":1461146400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461146400,"objectID":"b24f19b711d7c1f927d3b3787e4c375d","permalink":"/post/writing-markdown-latex/","publishdate":"2016-04-20T10:00:00Z","relpermalink":"/post/writing-markdown-latex/","section":"post","summary":"Content can be written using Markdown, LaTeX math, and Hugo Shortcodes. Additionally, HTML may be used for advanced formatting.\n","tags":["academic","hugo","tutorial"],"title":"Writing content with Markdown, LaTeX, and Shortcodes","type":"post"},{"authors":["M Scipioni","MF Santarelli","V Positano","L Landini"],"categories":null,"content":"","date":1460160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460160000,"objectID":"bed58a945648f9eeb37b86a9a0f94892","permalink":"/publication/medicon2016/","publishdate":"2016-04-09T00:00:00Z","relpermalink":"/publication/medicon2016/","section":"publication","summary":"In the present work a study is carried out in order to assess the efficiency of the direct reconstruction algorithms on noisy dynamic PET data. The study is performed via Monte Carlo simulations of a uniform cylindrical phantom whose emission values change in time according to a kinetic law. After generating the relevant projection data and properly adding the effects of different noise sources on them, the direct reconstruction and parametric estimation algorithm is applied. The resulting kinetic parameters and reconstructed images are then quantitatively evaluated with appropriate indexes. The simulation is repeated considering different sources of noise and different values of them. The results obtained allow us to affirm that the direct reconstruction algorithm tested maintains a good efficiency also in presence of noise.","tags":null,"title":"The Influence of Noise in Dynamic PET Direct Reconstruction","type":"publication"},{"authors":null,"categories":null,"content":"Abstract\nIn the present work a study is carried out in order to assess the efficiency of the direct reconstruction algorithms on noisy dynamic PET data. The study is performed via Monte Carlo simulations of a uniform cylindrical phantom whose emission values change in time according to a kinetic law. After generating the relevant projection data and properly adding the effects of different noise sources on them, the direct reconstruction and parametric estimation algorithm is applied. The resulting kinetic parameters and reconstructed images are then quantitatively evaluated with appropriate indexes. The simulation is repeated considering different sources of noise and different values of them. The results obtained allow us to affirm that the direct reconstruction algorithm tested maintains a good efficiency also in presence of noise.\nKeywords‚Äî dynamic positron emission tomography, direct reconstruction, kinetic analysis, compartmental model, noise\n","date":1459555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459555200,"objectID":"4819aec66024ca0f296066f17c9ebe6b","permalink":"/talk/medicon16/","publishdate":"2016-04-02T00:00:00Z","relpermalink":"/talk/medicon16/","section":"talk","summary":"Abstract\nIn the present work a study is carried out in order to assess the efficiency of the direct reconstruction algorithms on noisy dynamic PET data. The study is performed via Monte Carlo simulations of a uniform cylindrical phantom whose emission values change in time according to a kinetic law.","tags":null,"title":"The Influence of Noise in Dynamic PET Direct Reconstruction","type":"talk"},{"authors":null,"categories":null,"content":"Introduction\nDynamic positron emission tomography (PET) studies allow to quantify tissue-specific biochemical properties. Conventional pharmacokinetic analysis requires the voxel-wise time activity curve fitting performed on a sequence of independently reconstructed PET images. Direct parametric reconstruction methods combine emission image reconstruction and kinetic modeling into a single formula, estimating parametric images directly from raw data. In the present work a comparison between the two pharmacokinetic analysis methods is performed on simulated and clinical brain 18F[FDG] PET data.\nMethods\nMonte Carlo simulation includes 2D dynamic raw data generation of a brain phantom (gray and white matter) based on realistic kinetic parameter‚Äôs values. Attenuation and random counts effect are included. The Feng‚Äôs model is used to generate the input function. 20 realizations are analyzed, each including 24 time samples ranged from 10s to 600s.\nClinical\n2D 18F[FDG] PET data relevant to the brain of a patient (24 time frames) are acquired by GE PET/CT DRX scanner. The same ROI-based input function (covering the carotid) is used for both methods. The conventional estimation consists of a full OSEM reconstruction and one step of voxel-wise non linear least square parametric fitting with a 2 compartments-3k kinetic model. The direct algorithm is based on optimization transfer framework and performs, at each iteration, an EM-like dynamic image update and a pixel-wise penalized likelihood kinetic fitting using the same model as in conventional kinetic analysis. On simulated data we assess the goodness of direct method with nRMSE, normalizing the error on direct estimate with the one relevant to conventional estimate. Linear regression is then performed for each of the kinetic constants on simulated and clinical data.\nResults\nnRMSE values for K1, k2, k3 and Ki parameters are 0.7722¬±0.0024, 0.7847¬±0.0077, 1.0003¬±5.16e-05, 0.9960¬±1.3e-04, respectively. Regression analysis on simulated data (fig.1) gives following angular coefficients (and R value) for each k-parameter: 1.03(0.99), 0.94(0.94), 0.88(0.91), 1.03(0.99). The same test on clinical data (fig.2) gives: 0.88(0.87), 0.70(0.77), 0.72(0.80), 0.91(0.95).\nConclusions\nnRMSE analysis on simulated data shows a smaller error for directly estimated parameters. This could be due to a slight overestimation of k-values in conventional approach, particularly evident on clinical data, as it results from regression analysis.\n","date":1457481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1457481600,"objectID":"236501b7deec389f6e6b3b766aba6116","permalink":"/talk/emim16/","publishdate":"2016-03-09T00:00:00Z","relpermalink":"/talk/emim16/","section":"talk","summary":"Introduction\nDynamic positron emission tomography (PET) studies allow to quantify tissue-specific biochemical properties. Conventional pharmacokinetic analysis requires the voxel-wise time activity curve fitting performed on a sequence of independently reconstructed PET images.","tags":null,"title":"Pharmacokinetic analysis of dynamic PET data: comparison between direct parametric reconstruction and conventional indirect voxel-based estimation","type":"talk"}]