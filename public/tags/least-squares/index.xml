<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>least squares | MICHELE SCIPIONI</title>
    <link>/tags/least-squares/</link>
      <atom:link href="/tags/least-squares/index.xml" rel="self" type="application/rss+xml" />
    <description>least squares</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2020</copyright><lastBuildDate>Sun, 01 May 2016 11:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/foto_bw.jpg</url>
      <title>least squares</title>
      <link>/tags/least-squares/</link>
    </image>
    
    <item>
      <title>Fitting theoretical model to data in python</title>
      <link>/post/fitting-functions-to-data/</link>
      <pubDate>Sun, 01 May 2016 11:00:00 +0000</pubDate>
      <guid>/post/fitting-functions-to-data/</guid>
      <description>&lt;p&gt;There are several data fitting utilities available. We will focus on two:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;scipy.optimize&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lmfit.minimize&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using both those modules, you can fit any arbitrary function that you define and it is, also, possible to constrain given parameters during the fit. Another important aspect is that both packages come with useful
diagnostic tools.&lt;/p&gt;
&lt;h2 id=&#34;fitting-basics&#34;&gt;Fitting Basics&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;fitting&lt;/em&gt; we discuss here is an iterative process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, we define our &lt;strong&gt;desired function&lt;/strong&gt;, and calculate values given certain parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then we &lt;strong&gt;calculate the difference&lt;/strong&gt; between the initial and the new values&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final aim is to minimize this difference (specifically, we generally minimize the sum of the squares of these differences).&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;Several examples can be found at &lt;a href=&#34;http://www.scipy.org/Cookbook/FittingData&#34;&gt;http://www.scipy.org/Cookbook/FittingData&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Minimization is usually done by the method of &lt;strong&gt;least squares fitting&lt;/strong&gt;. There are several algorithms available for this minimization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The most common is the &lt;strong&gt;Levenberg-Marquardt&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Susceptible to finding &lt;em&gt;local minima&lt;/em&gt; instead of &lt;em&gt;global&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Fast&lt;/li&gt;
&lt;li&gt;Usually well-behaved for most functions&lt;/li&gt;
&lt;li&gt;By far the &lt;em&gt;most tested of methods&lt;/em&gt;, with many accompanying statistics implemented&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other methods include the &lt;strong&gt;Nelder-Mead&lt;/strong&gt;, &lt;strong&gt;L-BFGS-B&lt;/strong&gt;, and &lt;strong&gt;Simulated Annealing&lt;/strong&gt; algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;goodness-of-fit-gof&#34;&gt;Goodness-of-Fit (GoF)&lt;/h2&gt;
&lt;p&gt;There are several statistics that can help you determine the goodness-of-fit. Most commonly used are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reduced chi-squared&lt;/li&gt;
&lt;li&gt;Standard error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can get these and other tools for free with &lt;strong&gt;lmfit.minimize&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-1-fit-a-quadratic-curve-with-no-constraints&#34;&gt;Example 1: Fit a quadratic curve with no constraints&lt;/h2&gt;
&lt;p&gt;First, let&amp;rsquo;s try fitting a simple quadratic to some fake data:&lt;/p&gt;
&lt;p&gt;$$ y = ax^2 + bx + c $$&lt;/p&gt;
&lt;p&gt;What we will do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate some data for the example&lt;/li&gt;
&lt;li&gt;Define the function we wish to fit&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;scipy.optimize&lt;/strong&gt; to do the actual optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s assume the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The x-data is an array from -3 to 10&lt;/li&gt;
&lt;li&gt;The y-data is $x^2$, with some random noise added.&lt;/li&gt;
&lt;li&gt;Let&amp;rsquo;s put our initial guesses for the coefficients a,b,c into a list called p0 (for fit parameters)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

#Generate the arrays
xarray1=np.arange(-3,10,.2)
yarray1=xarray1**2

#Adding noise
yarray1+=np.random.randn(yarray1.shape[0])*2
p0=[2,2,2] #Our initial guesses for our fit parameters
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are dealing with a &lt;strong&gt;quadratic fit&lt;/strong&gt; we can use a &lt;em&gt;cheap &amp;amp; easy&lt;/em&gt; method &lt;em&gt;&lt;strong&gt;for polynomials (only)&lt;/strong&gt;&lt;/em&gt;: &lt;em&gt;scipy.polyfit()&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This method involves the least amount of setup while it simply outputs an array of the coefficients that best fit the data to the specified polynomial order.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
from scipy import polyfit
from scipy.optimize import leastsq as lsq
import matplotlib.pyplot as plt

# polyfit(x, y, deg)
fitcoeffs=polyfit(xarray1,yarray1,2)

print &amp;quot;Parameter fitted using polyfit&amp;quot;
print fitcoeffs
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Parameter fitted using polyfit
[ 1.00811611 -0.21729382  0.6272779 ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the function you want to fit, remembering that &lt;strong&gt;p&lt;/strong&gt; will be our array of initial guesses to the fit parameters, the coefficients &lt;strong&gt;a, b, c&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quadratic(p,x):
    y_out=p[0]*(x**2)+p[1]*x+p[2]
    return y_out

#Is the same as
#quadratic = lambda p,x: p[0]*(x**2)+p[1]*x+p[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we define a function that &lt;strong&gt;returns the difference&lt;/strong&gt; between the fit iteration value and the initial data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;quadraticerr = lambda p,x,y: quadratic(p,x)-y
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This difference or residual is the quantity that we will minimize with &lt;em&gt;scipy.optimize&lt;/em&gt;. To do so, we call the least-squares optimization routine with &lt;strong&gt;scipy.optimize.leastsq()&lt;/strong&gt; that stores the parameters you fit &lt;em&gt;in the zeroth element&lt;/em&gt; of the output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fitout=lsq(quadraticerr,p0[:],args=(xarray1,yarray1))
paramsout=fitout[0] #These are the fitted coefficients
covar=fitout[1] #This is the covariance matrix output

print(&#39;Fitted Parameters using scipy\&#39;s leastsq():\na = %.2f , b = %.2f , c = %.2f&#39;
% (paramsout[0],paramsout[1],paramsout[2]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fitted Parameters using scipy&#39;s leastsq():
a = 1.01 , b = -0.22 , c = 0.63
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to get an array values for the results, just call your function definition with the fitted parameters, while the residuals, of course, will just be their difference from the original data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fitarray1=quadratic(paramsout,xarray1)
residualarray1=fitarray1-yarray1

plt.rc(&#39;font&#39;,family=&#39;serif&#39;)
fig1=plt.figure(1)
frame1=fig1.add_axes((.1,.3,.8,.6))
    #xstart, ystart, xwidth, yheight --&amp;gt; units are fraction of the image from bottom left

xsmooth=np.linspace(xarray1[0],xarray1[-1])
plt.plot(xarray1,yarray1,&#39;.&#39;)
plt.plot(xsmooth,quadratic(paramsout,xsmooth))
frame1.set_xticklabels([]) #We will plot the residuals below, so no x-ticks on this plot
plt.title(&#39;Quadratic Fit Example&#39;)
plt.ylabel(&#39;y-data&#39;)
plt.grid(True)
frame1.annotate(&#39;$y$ = %.2f$\cdot x^2$+%.2f$\cdot x$+%.2f&#39;%(paramsout[0],paramsout[1],paramsout[2]), \
                xy=(.05,.95),xycoords=&#39;axes fraction&#39;,ha=&amp;quot;left&amp;quot;,va=&amp;quot;top&amp;quot;,bbox=dict(boxstyle=&amp;quot;round&amp;quot;, fc=&#39;1&#39;))

from matplotlib.ticker import MaxNLocator
plt.gca().yaxis.set_major_locator(MaxNLocator(prune=&#39;lower&#39;)) #Removes lowest ytick label

frame2=fig1.add_axes((.1,.1,.8,.2))

plt.plot(xarray1,quadratic(paramsout,xarray1)-yarray1)
plt.ylabel(&#39;Residuals&#39;)
plt.grid(True)

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/fitting-functions-to-data/output_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;example-2-more-complex-functions-with-constraints&#34;&gt;Example 2: More complex functions, with constraints&lt;/h2&gt;
&lt;p&gt;Often we want to set limits on the values that our fitted parameters can have, for example, to be sure that one of the parameters can&amp;rsquo;t be negative, etc.&lt;/p&gt;
&lt;p&gt;To do this, we can use &lt;em&gt;scipy.optimize.minimize()&lt;/em&gt; or another useful package could be &lt;em&gt;lmfit.minimize()&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We create an &lt;strong&gt;lmfit.Parameters()&lt;/strong&gt; object&lt;/li&gt;
&lt;li&gt;We can set limits for the parameters to be fit&lt;/li&gt;
&lt;li&gt;We can even tell some params not to vary at all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;Parameters()&lt;/strong&gt; object is then updated with every iteration.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use more real data for a typical real-world application: fitting a profile to spectral data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The data&lt;/strong&gt;: stacked velocity-amplitude spectra from a VLA observation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The functions&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;A modified Gaussian to include Hermite polynomials (approximations to skew and kurtosis)&lt;/li&gt;
&lt;li&gt;A double gaussian (gaus1 + gaus2 = gausTot)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The data have been downloaded from &lt;a href=&#34;https://science.nrao.edu/science/surveys/littlethings/data/wlm.html&#34;&gt;https://science.nrao.edu/science/surveys/littlethings/data/wlm.html&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pyfits

cube=pyfits.getdata(&#39;WLM_NA_ICL001.FITS&#39;)[0,:,:,:]
cubehdr=pyfits.getheader(&#39;WLM_NA_ICL001.FITS&#39;)

cdelt3=cubehdr[&#39;CDELT3&#39;]/1000.; crval3=cubehdr[&#39;CRVAL3&#39;]/1000.; crpix3=cubehdr[&#39;CRPIX3&#39;];
minvel=crval3+(-crpix3+1)*cdelt3; maxvel=crval3+(cube.shape[0]-crpix3)*cdelt3
chanwidth=abs(cdelt3)

stackspec=np.sum(np.sum(cube,axis=2),axis=1)
vels=np.arange(minvel,maxvel+int(cdelt3),cdelt3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The velocity array in the cube goes from positive to negative, so letâs reverse it to make the fitting go smoother.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vels=vels[::-1]
stackspec=stackspec[::-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to use the default &lt;em&gt;Marquardt-Levenberg algorithm&lt;/em&gt;.  Note that fitting results will depend quite a bit on what you give as initial guesses â ML finds LOCAL extrema quite well, but it doesnât necessarily find the global extrema.  In short, &lt;strong&gt;do your best to provide a good first guess&lt;/strong&gt; to the fit parameters.&lt;/p&gt;
&lt;p&gt;I said that we want to fit this dataset with a more complex model. Let me explain it a bit before to proced.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Gaussian&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$ f(x) = A e^{\frac{-g^2}{2}} $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;$ g = \frac{x-x_c}{\sigma} $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multiple Gaussians&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$ F(x) = \sum_i f_i(x) = A_1e^{\frac{-g_1^2}{2}} + A_2e^{\frac{-g_2^2}{2}} + \dots $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gauss-Hermite Polynomial&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$f(x) = Ae^{\frac{-g^2}{2}} [ 1+h_3(-\sqrt{3}g+\frac{2}{\sqrt{3}}g^3 ) + h_4 (\frac{\sqrt{6}}{4}-\sqrt{6}g^2+\frac{\sqrt{6}}{3}g^4)] $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H_3 â (Fisher) Skew: asymmetric component&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$\xi_1 \sim 4\sqrt{3}h_3$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H_4 â (Fisher) Kurtosis: how &amp;lsquo;fat&amp;rsquo; the tails are&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$xi_2 \sim 3+8\sqrt{6}h_4$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;$\xi_f = \xi_2-3$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;$\xi_f \sim 8\sqrt{6}h_4$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Set up the &lt;strong&gt;lmfit.Parameters()&lt;/strong&gt; and define the &lt;em&gt;Gauss-Hermite function&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lmfit import minimize, Parameters

p_gh=Parameters()
p_gh.add(&#39;amp&#39;,value=np.max(stackspec),vary=True);
p_gh.add(&#39;center&#39;,value=vels[50],min=np.min(vels),max=np.max(vels));
p_gh.add(&#39;sig&#39;,value=3*chanwidth,min=chanwidth,max=abs(maxvel-minvel));
p_gh.add(&#39;skew&#39;,value=0,vary=True,min=None,max=None);
p_gh.add(&#39;kurt&#39;,value=0,vary=True,min=None,max=None);

def gaussfunc_gh(paramsin,x):
    amp=paramsin[&#39;amp&#39;].value
    center=paramsin[&#39;center&#39;].value
    sig=paramsin[&#39;sig&#39;].value
    c1=-np.sqrt(3);
    c2=-np.sqrt(6)
    c3=2/np.sqrt(3);
    c4=np.sqrt(6)/3;
    c5=np.sqrt(6)/4
    skew=paramsin[&#39;skew&#39;].value
    kurt=paramsin[&#39;kurt&#39;].value
    g=(x-center)/sig
    gaustot_gh=amp*np.exp(-.5*g**2)*(1+skew*(c1*g+c3*g**3)+ kurt*(c5+c2*g**2+c4*(g**4)))

    return gaustot_gh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now do the same for the &lt;strong&gt;double gaussian&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Bounds&lt;/strong&gt;
amp : 10% of max to max&lt;br&gt;
center : velocity range&lt;br&gt;
disp : channel width to velocity range&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Double Gaussian   (labeled below as ..._2g)
p_2g=Parameters()
p_2g.add(&#39;amp1&#39;,value=np.max(stackspec)/2.,min=.1*np.max(stackspec),max=np.max(stackspec));
p_2g.add(&#39;center1&#39;,value=vels[50+10],min=np.min(vels),max=np.max(vels));
p_2g.add(&#39;sig1&#39;,value=2*chanwidth,min=chanwidth,max=abs(maxvel-minvel));
p_2g.add(&#39;amp2&#39;,value=np.max(stackspec)/2.,min=.1*np.max(stackspec),max=np.max(stackspec));
p_2g.add(&#39;center2&#39;,value=vels[50-10],min=np.min(vels),max=np.max(vels));
p_2g.add(&#39;sig2&#39;,value=3*chanwidth,min=chanwidth,max=abs(maxvel-minvel));

def gaussfunc_2g(paramsin,x):
    amp1=paramsin[&#39;amp1&#39;].value;
    amp2=paramsin[&#39;amp2&#39;].value;
    center1=paramsin[&#39;center1&#39;].value;
    center2=paramsin[&#39;center2&#39;].value;
    sig1=paramsin[&#39;sig1&#39;].value;
    sig2=paramsin[&#39;sig2&#39;].value;
    g1=(x-center1)/sig1
    g2=(x-center2)/sig2

    gaus1=amp1*np.exp(-.5*g1**2)
    gaus2=amp2*np.exp(-.5*g2**2)
    gaustot_2g=(gaus1+gaus2)
    return gaustot_2g
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now the functions that compute the difference between the fit iteration and data. In addition, define a function for a simple single gaussian.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gausserr_gh = lambda p,x,y: gaussfunc_gh(p,x)-y
gausserr_2g = lambda p,x,y: gaussfunc_2g(p,x)-y
gausssingle = lambda a,c,sig,x: a*np.exp(-.5*((x-c)/sig)**2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will minimize with &lt;em&gt;lmfit&lt;/em&gt;, in order to &lt;strong&gt;keep limits&lt;/strong&gt; on parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fitout_gh=minimize(gausserr_gh,p_gh,args=(vels,stackspec))
fitout_2g=minimize(gausserr_2g,p_2g,args=(vels,stackspec))

fitted_p_gh = fitout_gh.params
fitted_p_2g = fitout_2g.params

pars_gh=[fitout_gh.params[&#39;amp&#39;].value,
         fitout_gh.params[&#39;center&#39;].value,
         fitout_gh.params[&#39;sig&#39;].value,
         fitout_gh.params[&#39;skew&#39;].value,
         fitout_gh.params[&#39;kurt&#39;].value]
pars_2g=[fitted_p_2g[&#39;amp1&#39;].value,
         fitted_p_2g[&#39;center1&#39;].value,
         fitted_p_2g[&#39;sig1&#39;].value,
         fitted_p_2g[&#39;amp2&#39;].value,
         fitted_p_2g[&#39;center2&#39;].value,
         fitted_p_2g[&#39;sig2&#39;].value]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, if you want to create arrays and residuals of the final fit values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fit_gh=gaussfunc_gh(fitted_p_gh,vels)
fit_2g=gaussfunc_2g(fitted_p_2g,vels)

resid_gh=fit_gh-stackspec
resid_2g=fit_2g-stackspec

print(&#39;Fitted Parameters (Gaus+Hermite):\nAmp = %.2f , Center = %.2f , Disp = %.2f\nSkew = %.2f , Kurt = %.2f&#39; \
%(pars_gh[0],pars_gh[1],pars_gh[2],pars_gh[3],pars_gh[4]))

print(&#39;Fitted Parameters (Double Gaussian):\nAmp1 = %.2f , Center1 = %.2f , Sig1 = %.2f\nAmp2 = %.2f , Center2 = %.2f , Sig2 = %.2f&#39; \
%(pars_2g[0],pars_2g[1],pars_2g[2],pars_2g[3],pars_2g[4],pars_2g[5]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Fitted Parameters (Gaus+Hermite):
Amp = 178.66 , Center = -119.13 , Disp = 20.68
Skew = -0.12 , Kurt = -0.03
Fitted Parameters (Double Gaussian):
Amp1 = 189.58 , Center1 = -112.89 , Sig1 = 14.55
Amp2 = 91.58 , Center2 = -146.19 , Sig2 = 10.26
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig3=plt.figure(3,figsize=(15,10))
f1=fig3.add_axes((.1,.3,.8,.6))

plt.plot(vels,stackspec,&#39;k.&#39;)
pgh,=plt.plot(vels,fit_gh,&#39;b&#39;)
p2g,=plt.plot(vels,fit_2g,&#39;r&#39;)
p2ga,=plt.plot(vels,gausssingle(pars_2g[0],pars_2g[1],pars_2g[2],vels),&#39;-.&#39;,color=&#39;orange&#39;)
p2gb,=plt.plot(vels,gausssingle(pars_2g[3],pars_2g[4],pars_2g[5],vels),&#39;-.&#39;,color=&#39;green&#39;)
f1.set_xticklabels([]) #We will plot the residuals below, so no x-ticks on this plot
plt.title(&#39;Multiple Gaussian Fit Example&#39;)
plt.ylabel(&#39;Amplitude (Some Units)&#39;)
f1.legend([pgh,p2g,p2ga,p2gb],[&#39;Gaus-Hermite&#39;,&#39;2-Gaus&#39;,&#39;Comp. 1&#39;,&#39;Comp2&#39;],prop={&#39;size&#39;:10},loc=&#39;center left&#39;)

from matplotlib.ticker import MaxNLocator
plt.gca().yaxis.set_major_locator(MaxNLocator(prune=&#39;lower&#39;)) #Removes lowest ytick label

f1.annotate(&#39;Gauss-Hermite:\nAmp = %.2f\nCenter = %.2f\n$\sigma$ = %.2f\nH3 = %.2f\nH4 = %.2f&#39; \
    %(pars_gh[0],pars_gh[1],pars_gh[2],pars_gh[3],pars_gh[4]),xy=(.05,.95), \
    xycoords=&#39;axes fraction&#39;,ha=&amp;quot;left&amp;quot;, va=&amp;quot;top&amp;quot;, \
bbox=dict(boxstyle=&amp;quot;round&amp;quot;, fc=&#39;1&#39;),fontsize=10)
f1.annotate(&#39;Double Gaussian:\nAmp$_1$ = %.2f\nAmp$_2$ = %.2f\nCenter$_1$ = %.2f\nCenter$_2$ = %.2f\n$\sigma_1$ = %.2f\n$\sigma_2$ = %.2f&#39; \
    %(pars_2g[0],pars_2g[3],pars_2g[1],pars_2g[4],pars_2g[2],pars_2g[5]),xy=(.95,.95), \
    xycoords=&#39;axes fraction&#39;,ha=&amp;quot;right&amp;quot;, va=&amp;quot;top&amp;quot;, \
    bbox=dict(boxstyle=&amp;quot;round&amp;quot;, fc=&#39;1&#39;),fontsize=10)

f2=fig3.add_axes((.1,.1,.8,.2))

resgh,res2g,=plt.plot(vels,resid_gh,&#39;k--&#39;,vels,resid_2g,&#39;k&#39;)

plt.ylabel(&#39;Residuals&#39;)
plt.xlabel(&#39;Velocity (km s$^{-1}$)&#39;)
f2.legend([resgh,res2g],[&#39;Gaus-Hermite&#39;,&#39;2-Gaus&#39;],numpoints=4,prop={&#39;size&#39;:9},loc=&#39;upper left&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/fitting-functions-to-data/output_26_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
