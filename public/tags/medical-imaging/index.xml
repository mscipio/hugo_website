<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>medical imaging | MICHELE SCIPIONI</title>
    <link>/tags/medical-imaging/</link>
      <atom:link href="/tags/medical-imaging/index.xml" rel="self" type="application/rss+xml" />
    <description>medical imaging</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2020</copyright><lastBuildDate>Tue, 14 Nov 2017 13:49:34 +0100</lastBuildDate>
    <image>
      <url>/img/foto_bw.jpg</url>
      <title>medical imaging</title>
      <link>/tags/medical-imaging/</link>
    </image>
    
    <item>
      <title>Creation of a GitHub repository for lectures material</title>
      <link>/post/updated-lectures-repository/</link>
      <pubDate>Tue, 14 Nov 2017 13:49:34 +0100</pubDate>
      <guid>/post/updated-lectures-repository/</guid>
      <description>&lt;h4 id=&#34;new-github-repository-with-material-from-given-lectures-update-nov-2017&#34;&gt;New GitHub repository with material from given lectures (Update Nov 2017)&lt;/h4&gt;
&lt;p&gt;During my Ph.D. at the University of Pisa, I have had the chance to work as a Graduate Teaching Assistant for the following classes:&lt;/p&gt;
&lt;h3 id=&#34;2017&#34;&gt;2017&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Ing-Inf/06]&lt;/strong&gt;: &lt;span style=&#34;color:blue&#34;&gt;&lt;b&gt;Biomedical Imaging&lt;/b&gt;&lt;/span&gt; (&lt;em&gt;&lt;strong&gt;6 CFU&lt;/strong&gt;&lt;/em&gt;) &lt;span style=&#34;color:red&#34;&gt;&lt;b&gt;(ITA)&lt;/b&gt;&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/mscipio/Lectures/tree/master/2016/University%20of%20Pisa/Biomedical%20Imaging%20Course/Tracer%20Kinetic%20Modeling%20in%20PET%20dynamic%20imaging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Iterative reconstruction techniques for emission tomography imaging: ML-EM, OS-EM, and MAP-OSL-EM&lt;/em&gt;&lt;/a&gt; 
&lt;a href=&#34;https://github.com/mscipio/Lectures/blob/master/2017/University%20of%20Pisa/Biomedical%20Imaging%20Course/Iterative%20Reconstruction%20in%20Emission%20Tomography/Metodi_Iterativi_10_11_2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;[Download slides]**&lt;/strong&gt;&lt;/a&gt;
&lt;a href=&#34;https://github.com/mscipio/Lectures/blob/master/2017/University%20of%20Pisa/Biomedical%20Imaging%20Course/Iterative%20Reconstruction%20in%20Emission%20Tomography/Ricostruzione_Iterativa_in_Tomografia_2017_2018.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Lecture notes]**&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2016&#34;&gt;2016&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[Ing-Inf/06]&lt;/strong&gt;: &lt;span style=&#34;color:blue&#34;&gt;&lt;b&gt;Biomedical Imaging&lt;/b&gt;&lt;/span&gt; (&lt;em&gt;&lt;strong&gt;6 CFU&lt;/strong&gt;&lt;/em&gt;) &lt;span style=&#34;color:red&#34;&gt;&lt;b&gt;(ITA)&lt;/b&gt;&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/mscipio/Lectures/tree/master/2016/University%20of%20Pisa/Biomedical%20Imaging%20Course/Tracer%20Kinetic%20Modeling%20in%20PET%20dynamic%20imaging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Iterative reconstruction techniques for emission tomography imaging: ML-EM, OS-EM, and MAP-OSL-EM&lt;/em&gt;&lt;/a&gt; 
&lt;a href=&#34;data/teaching/Iterative_reconstruction_of_tomographic_images_28_10_2016.pdf%22&#34;&gt;**[Download slides]**&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/mscipio/Lectures/tree/master/2016/University%20of%20Pisa/Biomedical%20Imaging%20Course/Iterative%20Reconstruction%20in%20Emission%20Tomography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Introduction to kinetic modeling for emission tomography: focus on compartmental models (meaning, use, and interpratation)&lt;/em&gt;&lt;/a&gt; 
&lt;a href=&#34;https://github.com/mscipio/Lectures/blob/master/2016/University%20of%20Pisa/Biomedical%20Imaging%20Course/Tracer%20Kinetic%20Modeling%20in%20PET%20dynamic%20imaging/Kinetic_Modeling_01_12_2016.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;[Part1]**&lt;/strong&gt;&lt;/a&gt; 
&lt;a href=&#34;https://github.com/mscipio/Lectures/blob/master/2016/University%20of%20Pisa/Biomedical%20Imaging%20Course/Tracer%20Kinetic%20Modeling%20in%20PET%20dynamic%20imaging/Kinetic_Modeling_02_12_2016.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Part2]**&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Blind Source Separation (BSS) with the Shogun Machine Learning Toolbox</title>
      <link>/post/bss-shogun-python/</link>
      <pubDate>Sun, 01 May 2016 11:00:00 +0000</pubDate>
      <guid>/post/bss-shogun-python/</guid>
      <description>&lt;h4 id=&#34;strongly-inspired-by-an-article-by-kevin-hughes-httpsgithubcomkevinhughes27tabrepositories&#34;&gt;Strongly inspired by an article by Kevin Hughes (&lt;em&gt;&lt;a href=&#34;https://github.com/kevinhughes27?tab=repositories&#34;&gt;https://github.com/kevinhughes27?tab=repositories&lt;/a&gt;&lt;/em&gt;)&lt;/h4&gt;
&lt;p&gt;Today I am going to show you how we can do Blind Source Separation (BSS) using algorithms available in the Shogun Machine Learning Toolbox. What is &lt;strong&gt;Blind Source Separation&lt;/strong&gt;? &lt;em&gt;BSS&lt;/em&gt; is the separation of a set of source signals from a set of mixed signals.&lt;/p&gt;
&lt;p&gt;My favorite example of this problem is known as the &lt;strong&gt;cocktail party problem&lt;/strong&gt; where a number of people are talking simultaneously and we want to separate each persons speech so we can listen to it separately. Now the caveat with this type of approach is that we need as many mixtures as we have source signals or in terms of the cocktail party problem &lt;em&gt;we need as many microphones as people talking in the room&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s get started. This example is going to be in Python and the first thing we are going to need to do is &lt;em&gt;load some audio files&lt;/em&gt;. To make things a bit easier further on in this example I&amp;rsquo;m going to wrap the basic scipy wav file reader and add some additional functionality. First I added a case to handle converting stereo wav files back into mono wav files and secondly this loader takes a desired sample rate and resamples the input to match. This is important because when we mix the two audio signals they need to have the same sample rate. &lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.io import wavfile
from scipy.signal import resample

def load_wav(filename,samplerate=44100):

    # load file
    rate, data = wavfile.read(filename)

    # convert stereo to mono
    if len(data.shape) &amp;gt; 1:
        data = data[:,0]/2 + data[:,1]/2

    # re-interpolate samplerate    
    ratio = float(samplerate) / float(rate)
    data = resample(data, len(data) * ratio)

    return samplerate, data.astype(np.int16)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we&amp;rsquo;re going to need a way to play the audio files we&amp;rsquo;re working with (otherwise this wouldn&amp;rsquo;t be very exciting at all would it?). In the next bit of code I&amp;rsquo;ve defined a wavPlayer class that takes the signal and the sample rate and then creates a nice HTML5 webplayer right inline with the notebook.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#import StringIO
import base64
import struct  

from IPython.core.display import HTML

def wavPlayer(data, rate):
    &amp;quot;&amp;quot;&amp;quot; will display html 5 player for compatible browser
    The browser need to know how to play wav through html5.
    there is no autoplay to prevent file playing when the browser opens
    Adapted from SciPy.io. and
    github.com/Carreau/posts/blob/master/07-the-sound-of-hydrogen.ipynb
    &amp;quot;&amp;quot;&amp;quot;

    buffer = six.moves.StringIO()
    buffer.write(b&#39;RIFF&#39;)
    buffer.write(b&#39;\x00\x00\x00\x00&#39;)
    buffer.write(b&#39;WAVE&#39;)

    buffer.write(b&#39;fmt &#39;)
    if data.ndim == 1:
        noc = 1
    else:
        noc = data.shape[1]
    bits = data.dtype.itemsize * 8
    sbytes = rate*(bits // 8)*noc
    ba = noc * (bits // 8)
    buffer.write(struct.pack(&#39;&amp;lt;ihHIIHH&#39;, 16, 1, noc, rate, sbytes, ba, bits))

    # data chunk
    buffer.write(b&#39;data&#39;)
    buffer.write(struct.pack(&#39;&amp;lt;i&#39;, data.nbytes))

    if data.dtype.byteorder == &#39;&amp;gt;&#39; or (data.dtype.byteorder == &#39;=&#39; and sys.byteorder == &#39;big&#39;):
        data = data.byteswap()

    buffer.write(data.tostring())
    # return buffer.getvalue()
    # Determine file size and place it in correct
    # position at start of the file.
    size = buffer.tell()
    buffer.seek(4)
    buffer.write(struct.pack(&#39;&amp;lt;i&#39;, size-8))

    val = buffer.getvalue()

    src = &amp;quot;&amp;quot;&amp;quot;
    &amp;lt;head&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=utf-8&amp;quot;&amp;gt;
    &amp;lt;title&amp;gt;Simple Test&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;

    &amp;lt;body&amp;gt;
    &amp;lt;audio controls=&amp;quot;controls&amp;quot; style=&amp;quot;width:600px&amp;quot; &amp;gt;
      &amp;lt;source controls src=&amp;quot;data:audio/wav;base64,{base64}&amp;quot; type=&amp;quot;audio/wav&amp;quot; /&amp;gt;
      Your browser does not support the audio element.
    &amp;lt;/audio&amp;gt;
    &amp;lt;/body&amp;gt;
    &amp;quot;&amp;quot;&amp;quot;.format(base64=base64.encodestring(val))
    display(HTML(src))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we can load and play wav files we actually need some wav files! I found the sounds from Starcraft to be a great source of wav files because they&amp;rsquo;re short, interesting and remind me of my childhood. You can download Starcraft wav files here: &lt;a href=&#34;http://wavs.unclebubby.com/computer/starcraft/&#34;&gt;http://wavs.unclebubby.com/computer/starcraft/&lt;/a&gt; among other places on the web or from your Starcraft install directory (come on I know it&amp;rsquo;s still there).&lt;/p&gt;
&lt;p&gt;Another good source of data (although lets be honest less cool) is ICA central and various other more academic data sets: &lt;a href=&#34;http://perso.telecom-paristech.fr/~cardoso/icacentral/base_multi.html&#34;&gt;http://perso.telecom-paristech.fr/~cardoso/icacentral/base_multi.html&lt;/a&gt;. Note that for lots of these data sets the data will be mixed already so you&amp;rsquo;ll be able to skip the next few steps.&lt;/p&gt;
&lt;p&gt;Okay lets load up an audio file. I chose the Terran Battlecruiser saying &amp;ldquo;Good Day Commander&amp;rdquo;. In addition to the creating a wavPlayer I also plotted the data using Matplotlib (and tried my best to have the graph length match the HTML player length). Have a listen!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change to the shogun-data directoy
import os
os.chdir(&#39;../files&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%pylab inline
import pylab as pl
import numpy as np

# load
fs1,s1 = load_wav(&#39;audio1.wav&#39;) # Terran Marine - &amp;quot;You want a piece of me, boy?&amp;quot;

# plot
pl.figure(figsize=(7,2))
pl.plot(s1)
pl.title(&#39;Signal 1&#39;)
pl.show()

# player
wavPlayer(s1, fs1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_8_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/audio1.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now let&amp;rsquo;s load a second audio clip:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load
fs2,s2 = load_wav(&#39;audio2.wav&#39;) # Terran Battlecruiser - &amp;quot;Good day, commander.&amp;quot;

# plot
pl.figure(figsize=(6.75,2))
pl.plot(s2)
pl.title(&#39;Signal 2&#39;)
pl.show()

# player
wavPlayer(s2, fs2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/audio2.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;and a third audio clip:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load
fs3,s3 = load_wav(&#39;audio3.wav&#39;) # Protoss Zealot - &amp;quot;My life for Aiur!&amp;quot;

# plot
pl.figure(figsize=(6.75,2))
pl.plot(s3)
pl.title(&#39;Signal 3&#39;)
pl.show()

# player
wavPlayer(s3, fs3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/audio3.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we&amp;rsquo;ve got our audio files loaded up into our example program. The next thing we need to do is mix them together!&lt;/p&gt;
&lt;p&gt;First another nuance - what if the audio clips aren&amp;rsquo;t the same lenth? The solution I came up with for this was to simply resize them all to the length of the longest signal, the extra length will just be filled with zeros so it won&amp;rsquo;t affect the sound.&lt;/p&gt;
&lt;p&gt;The signals are mixed by creating a mixing matrix $A$ and taking the dot product of $A$ with the signals $S$.&lt;/p&gt;
&lt;p&gt;Afterwards I plot the mixed signals and create the wavPlayers, have a listen!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Adjust for different clip lengths
fs = fs1
length = max([len(s1), len(s2), len(s3)])

s1.resize((length,1), refcheck=False)
s2.resize((length,1), refcheck=False)
s3.resize((length,1), refcheck=False)

&amp;quot;&amp;quot;&amp;quot;
The function numpy.c_ concatenates the numpy arrays given as input.
The method numpy_array.T is the transpose operation that allow us
to prepare an input source matrix of the right size (3, length),
according to the chosen mixing matrix (3,3).
&amp;quot;&amp;quot;&amp;quot;
S = (np.c_[s1, s2, s3]).T

# Mixing Matrix
#A = np.random.uniform(size=(3,3))
#A = A / A.sum(axis=0)
A = np.array([[1, 0.5, 0.5],
              [0.5, 1, 0.5],
              [0.5, 0.5, 1]])
print &#39;Mixing Matrix:&#39;
print A.round(2)

# Mixed Signals
X = np.dot(A,S)

# Exploring Mixed Signals
for i in range(X.shape[0]):
    pl.figure(figsize=(6.75,2))
    pl.plot((X[i]).astype(np.int16))
    pl.title(&#39;Mixed Signal %d&#39; % (i+1))
    pl.show()
    wavPlayer((X[i]).astype(np.int16), fs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mixing Matrix:
[[ 1.   0.5  0.5]
 [ 0.5  1.   0.5]
 [ 0.5  0.5  1. ]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_14_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/mixaudio1.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_14_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/mixaudio2.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_14_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/mixaudio3.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now before we can work on separating these signals we need to get the data ready for Shogun.
Thankfully this is pretty easy!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from shogun.Features  import RealFeatures

# Convert to features for shogun
mixed_signals = RealFeatures((X).astype(np.float64))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets &lt;strong&gt;unmix&lt;/strong&gt; those signals!&lt;/p&gt;
&lt;p&gt;In this example I&amp;rsquo;m going to use an &lt;strong&gt;Independent Component Analysis (ICA)&lt;/strong&gt; algorithm called &lt;em&gt;JADE&lt;/em&gt;. JADE is one of the ICA algorithms available in Shogun and it works by performing &lt;em&gt;&lt;strong&gt;Approximate Joint Diagonalization&lt;/strong&gt;&lt;/em&gt; (AJD) on a 4th order cumulant tensor. I&amp;rsquo;m not going to go into a lot of detail on how JADE works behind the scenes but here is the reference for the original paper:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cardoso, J. F., &amp;amp; Souloumiac, A. (1993). Blind beamforming for non-Gaussian signals. In IEEE Proceedings F (Radar and Signal Processing) (Vol. 140, No. 6, pp. 362-370). IET Digital Library.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Shogun also has several other ICA algorithms including the Second Order Blind Identification (SOBI) algorithm, FFSep, JediSep, UWedgeSep and FastICA. All of the algorithms inherit from the ICAConverter base class and share some common methods for setting an intial guess for the mixing matrix, retrieving the final mixing matrix and getting/setting the number of iterations to run and the desired convergence tolerance. Some of the algorithms have additional getters for intermediate calculations, for example Jade has a method for returning the 4th order cumulant tensor while the &amp;ldquo;Sep&amp;rdquo; algorithms have a getter for the time lagged covariance matrices. Check out the source code on GitHub or the Shogun docs for more details!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from shogun.Converter import Jade

# Separating with JADE
jade = Jade()
signals = jade.apply(mixed_signals)

S_ = signals.get_feature_matrix()

A_ = jade.get_mixing_matrix()
A_ = A_ / A_.sum(axis=0)

print &#39;Estimated Mixing Matrix:&#39;
print A_
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Estimated Mixing Matrix:
[[ 0.25098835  0.49907993  0.24442146]
 [ 0.26235007  0.25543257  0.53186567]
 [ 0.48666158  0.2454875   0.22371287]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thats all there is to it!&lt;/p&gt;
&lt;p&gt;Check out how nicely those signals have been separated and have a listen!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Show separation results

# Separated Signal i
gain = 4000
for i in range(S_.shape[0]):
    pl.figure(figsize=(6.75,2))
    pl.plot((gain*S_[i]).astype(np.int16))
    pl.title(&#39;Separated Signal %d&#39; % (i+1))
    pl.show()
    wavPlayer((gain*S_[i]).astype(np.int16), fs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/unmixaudio1.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_20_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/unmixaudio2.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/bss-shogun-python/output_20_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;div ID=&#34;div-2&#34; style=&#34;display: table; margin: 0 auto;&#34;&gt;
&lt;audio controls=&#34;controls&#34; &gt;
  &lt;source type=&#34;audio/wav&#34; src=&#34;../../data/posts/unmixaudio3.wav&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;BSS isn&amp;rsquo;t only useful for working with Audio, it is also useful for image processing and pre-processing other forms of high dimensional data.
Have a google for ICA and machine learning if you want to learn more, but we will sure come back in the future on this topic!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to read DICOM files into Python</title>
      <link>/post/read_dicom_files_in_python/</link>
      <pubDate>Sun, 01 May 2016 11:00:00 +0000</pubDate>
      <guid>/post/read_dicom_files_in_python/</guid>
      <description>&lt;h2 id=&#34;dataset&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Dataset is the main object you will work with directly. Dataset is derived from pythonâs &lt;strong&gt;dict&lt;/strong&gt;, so it inherits (and overrides some of) the methods of dict. In other words it is a collection of key:value pairs, where the key value is the DICOM (group,element) tag (as a Tag object, described below), and the value is a DataElement instance (also described below).&lt;/p&gt;
&lt;p&gt;A dataset could be created directly, but you will usually get one by reading an existing DICOM file (it could be a .dcm or a .img file):&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import dicom
ds = dicom.read_file(&amp;quot;1111.img&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can display the entire dataset by simply printing its string (str or repr) value:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(0008, 0000) Group Length             UL: 482
(0008, 0005) Specific Character Set   CS: &#39;ISO_IR 100&#39;
(0008, 0008) Image Type               CS: [&#39;ORIGINAL&#39;, &#39;PRIMARY&#39;]
(0008, 0012) Instance Creation Date   DA: &#39;20150710&#39;
(0008, 0013) Instance Creation Time   TM: &#39;152456&#39;
(0008, 0014) Instance Creator UID     UI: 1.2.840.113619.1.131
(0008, 0016) SOP Class UID            UI: Positron Emission Tomography Image Storage
(0008, 0018) SOP Instance UID         UI: 1.2.840.113619.2.131.1611270158.1436534696
(0008, 0020) Study Date               DA: &#39;20150702&#39;
(0008, 0021) Series Date              DA: &#39;20150702&#39;
(0008, 0022) Acquisition Date         DA: &#39;20150702&#39;
(0008, 0023) Content Date             DA: &#39;20150710&#39;
(0008, 0030) Study Time               TM: &#39;090706&#39;
(0008, 0031) Series Time              TM: &#39;091216&#39;
(0008, 0032) Acquisition Time         TM: &#39;094216&#39;
(0008, 0033) Content Time             TM: &#39;152456&#39;
(0008, 0050) Accession Number         SH: &#39;120.116962&#39;
(0008, 0060) Modality                 CS: &#39;PT&#39;
(0008, 0070) Manufacturer             LO: &#39;GE MEDICAL SYSTEMS&#39;
(0008, 0080) Institution Name         LO: &#39;FTGM&#39;
(0008, 0090) Referring Physician Name PN: &#39;&#39;
(0008, 1010) Station Name             SH: &#39;pet94ct&#39;
(0008, 1030) Study Description        LO: &#39;&#39;
(0008, 103e) Series Description       LO: &#39;e+1 NEUROTOTEM&#39;
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;access-header-data-elements&#34;&gt;&lt;strong&gt;Access header data elements&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;You can access specific data elements by name (DICOM âkeywordâ) or by DICOM tag number:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds.ManufacturerModelName
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;Discovery RX&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds[0x0008,0x1090].value
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;Discovery RX&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the latter case (using the tag number directly) a DataElement instance is returned, so the .value must be used to get the value.&lt;/p&gt;
&lt;p&gt;You can also set values by name (DICOM keyword) or tag number:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds.PatientID = &amp;quot;12345&amp;quot;
ds.SeriesNumber = 5
ds[0x10,0x10].value = &#39;Test&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The use of names is possible because PyDicom intercepts requests for member variables, and checks if they are in the DICOM dictionary. It translates the keyword to a (group,element) number and returns the corresponding value for that key if it exists.&lt;/p&gt;
&lt;p&gt;If you donât remember or know the exact tag name, Dataset provides a handy dir() method, useful during interactive sessions at the python prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds.dir(&amp;quot;pat&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&#39;AdditionalPatientHistory&#39;,
 &#39;ImageOrientationPatient&#39;,
 &#39;ImagePositionPatient&#39;,
 &#39;PatientAge&#39;,
 &#39;PatientBirthDate&#39;,
 &#39;PatientGantryRelationshipCodeSequence&#39;,
 &#39;PatientID&#39;,
 &#39;PatientName&#39;,
 &#39;PatientOrientationCodeSequence&#39;,
 &#39;PatientOrientationModifierCodeSequence&#39;,
 &#39;PatientPosition&#39;,
 &#39;PatientSex&#39;,
 &#39;PatientSize&#39;,
 &#39;PatientWeight&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;dir will return any DICOM tag names in the dataset that have the specified string anywhere in the name (case insensitive).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;
Calling dir with no string will list all tag names available in the dataset.
You can also see all the names that pydicom knows about by viewing the _dicom_dict.py file. You could &amp;gt;modify that file to add tags that pydicom doesnât already know about.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under the hood, Dataset stores a DataElement object for each item, but when accessed by name (e.g. ds.PatientName) only the value of that DataElement is returned. If you need the whole DataElement (see the DataElement class discussion), you can use Datasetâs data_element() method or access the item using the tag number:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_element = ds.data_element(&amp;quot;PatientsName&amp;quot;)  # or data_element = ds[0x10,0x10]
data_element.VR, data_element.value
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&#39;PN&#39;, &#39;Test&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;dataelement&#34;&gt;&lt;em&gt;&lt;strong&gt;DataElement&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;The DataElement class is not usually used directly in user code, but is used extensively by Dataset. DataElement is a simple object which stores the following things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;tag&lt;/strong&gt; â a DICOM tag (as a Tag object)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VR&lt;/strong&gt; â DICOM value representation â various number and string formats, etc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VM&lt;/strong&gt; â value multiplicity. This is 1 for most DICOM tags, but can be multiple, e.g. for coordinates. You do not have to specify this, the DataElement class keeps track of it based on value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;value&lt;/strong&gt; â the actual value. A regular value like a number or string (or list of them), or a Sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To check for the existence of a particular tag before using it, use the in keyword:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;PatientName&amp;quot; in ds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove a data element from the dataset, use del:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;del ds.SoftwareVersions   # or del ds[0x0018, 0x1020]
&amp;quot;SoftwareVersions&amp;quot; in ds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;False
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;tag&#34;&gt;&lt;strong&gt;TAG&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Using DICOM keywords is the recommended way to access data elements, but you can also use the tag numbers directly. The Tag class is derived from pythonâs &lt;em&gt;int&lt;/em&gt;, so in effect, it is just a number with some extra behaviour:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tag enforces that the DICOM tag fits in the expected 4-byte (group,element)&lt;/li&gt;
&lt;li&gt;A Tag instance can be created from an int or from a tuple containing the (group,element) separately&lt;/li&gt;
&lt;li&gt;Tag has properties group and element (or elem) to return the group and element portions&lt;/li&gt;
&lt;li&gt;The is_private property checks whether the tag represents a private tag (i.e. if group number is odd).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from dicom.tag import Tag
t1=Tag(0x00100010) # all of these are equivalent
t2=Tag(0x10,0x10)
t3=Tag((0x10, 0x10))
print t1

t1==t2, t1==t3
(True, True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(0010, 0010)

(True, True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;access-image-data&#34;&gt;&lt;strong&gt;Access image data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;DICOM Sequences are turned into python lists or strings. Items in the sequence are referenced by number, beginning at index 0 as per python convention. &amp;ldquo;Sequence&amp;rdquo; data type is derived from pythonâs &lt;em&gt;list&lt;/em&gt;. The only added functionality is to &lt;strong&gt;make string representations prettier&lt;/strong&gt;. Otherwise all the usual methods of list like item selection, append, etc. are available. To work with pixel data, the raw bytes are available through the usual tag:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_data = ds.PixelData
print type(image_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;type &#39;str&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then calculate the total dimensions of the NumPy array which are equal to (Number of pixel rows in a slice) x (Number of pixel columns in a slice) x (Number of slices) along the x, y, and z cartesian axes. In this example we are dealing with just a single slice DICOM file, so z=1.&lt;/p&gt;
&lt;p&gt;Lastly, we use the PixelSpacing and SliceThickness attributes to calculate the spacing between pixels in the three axes. We store the array dimensions in ConstPixelDims and the spacing in ConstPixelSpacing.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

ConstPixelDims = (int(ds.Rows), int(ds.Columns))
ConstPixelSpacing = (float(ds.PixelSpacing[0]), float(ds.PixelSpacing[1]), float(ds.SliceThickness))

print ConstPixelDims
print ConstPixelSpacing
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(128, 128)
(3.125, 3.125, 3.27)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we simply use numpy.arange, ConstPixelDims, and ConstPixelSpacing to calculate axes for this array:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = np.arange(0.0, (ConstPixelDims[0]+1)*ConstPixelSpacing[0], ConstPixelSpacing[0])
y = np.arange(0.0, (ConstPixelDims[1]+1)*ConstPixelSpacing[1], ConstPixelSpacing[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, comes the last pydicom part:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The array is sized based on &#39;ConstPixelDims&#39;
ArrayDicom = np.zeros(ConstPixelDims, dtype=ds.pixel_array.dtype)
ArrayDicom[:,:] = ds.pixel_array
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
from matplotlib import pyplot, cm

pyplot.figure(dpi=300)
pyplot.axes().set_aspect(&#39;equal&#39;)
pyplot.set_cmap(pyplot.gray())
pyplot.pcolormesh(x, y, np.flipud(ArrayDicom[:, :]))
pyplot.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/posts/read_dicom_files_in_python/output_27_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
